# LangChain AWS Integration Package - Complete Documentation

This is the comprehensive documentation for `langchain-aws`, the official AWS integration package for LangChain. This package provides components for various AWS services including Bedrock, SageMaker, Kendra, Neptune, and more.

## Package Overview

The `langchain-aws` package provides LangChain components for AWS services, replacing and expanding upon existing AWS components in the `langchain-community` package. It includes support for LLMs, chat models, embeddings, retrievers, and graph databases.

## Installation

```bash
pip install langchain-aws
```

Note: All integrations assume AWS credentials are properly configured to connect with AWS services.

## Core Components

### Chat Models

#### ChatBedrock
The primary chat model interface for Amazon Bedrock foundation models.

```python
from langchain_aws import ChatBedrock

llm = ChatBedrock(
    model_id="anthropic.claude-v2:1",
    region_name="us-east-1"
)
response = llm.invoke("Hello! How are you today?")
```

**Supported Foundation Models:**
- **Anthropic**: Claude 3.5 Sonnet, Claude 3 Opus, Claude 3 Sonnet, Claude 3 Haiku, Claude 2.1, Claude 2.0, Claude Instant
- **AI21 Labs**: Jurassic-2 Ultra, Jurassic-2 Mid
- **Cohere**: Command Light, Command, Command R, Command R+
- **Meta**: Llama 2 Chat 13B, Llama 2 Chat 70B, Llama 3 8B Instruct, Llama 3 70B Instruct
- **Amazon**: Titan Text G1 - Express, Titan Text G1 - Lite
- **Mistral AI**: Mistral 7B Instruct, Mixtral 8x7B Instruct, Mistral Large

**Key Features:**
- Streaming responses
- Tool/function calling
- Multi-modal support (text and images)
- Custom inference parameters
- Response streaming
- Async support

**Configuration Parameters:**
```python
llm = ChatBedrock(
    model_id="anthropic.claude-3-sonnet-20240229-v1:0",
    model_kwargs={
        "max_tokens": 1000,
        "temperature": 0.7,
        "top_p": 0.9,
        "stop_sequences": ["\n\nHuman:"]
    },
    streaming=True,
    callbacks=[],
    region_name="us-east-1"
)
```

#### ChatBedrockConverse
Unified conversational interface for Bedrock models using the Converse API.

```python
from langchain_aws import ChatBedrockConverse

llm = ChatBedrockConverse(
    model="anthropic.claude-3-sonnet-20240229-v1:0",
    temperature=0,
    max_tokens=None,
    region_name="us-east-1"
)
```

**Benefits over ChatBedrock:**
- Unified API across all Bedrock models
- Consistent message formatting
- Better tool calling support
- Standardized response format

### LLMs

#### BedrockLLM
Legacy interface for Amazon Bedrock language models.

```python
from langchain_aws import BedrockLLM

llm = BedrockLLM(
    model_id="anthropic.claude-v2:1",
    model_kwargs={
        "max_tokens_to_sample": 1000,
        "temperature": 0.1
    }
)
response = llm.invoke("The meaning of life is")
```

#### SagemakerEndpoint
Interface for custom models deployed on Amazon SageMaker endpoints.

```python
from langchain_aws import SagemakerEndpoint
from langchain_aws.llms.sagemaker_endpoint import LLMContentHandler

class CustomContentHandler(LLMContentHandler):
    content_type = "application/json"
    accepts = "application/json"

    def transform_input(self, prompt: str, model_kwargs: dict) -> bytes:
        input_str = json.dumps({
            "inputs": prompt, 
            "parameters": model_kwargs
        })
        return input_str.encode('utf-8')
    
    def transform_output(self, output: bytes) -> str:
        response_json = json.loads(output.read().decode("utf-8"))
        return response_json["generated_text"]

content_handler = CustomContentHandler()

llm = SagemakerEndpoint(
    endpoint_name="my-custom-model-endpoint",
    region_name="us-west-2",
    content_handler=content_handler,
    model_kwargs={"temperature": 0.7}
)
```

### Embeddings

#### BedrockEmbeddings
Embedding models from Amazon Bedrock.

```python
from langchain_aws import BedrockEmbeddings

embeddings = BedrockEmbeddings(
    model_id="amazon.titan-embed-text-v1",
    region_name="us-east-1"
)

# Single query
query_embedding = embeddings.embed_query("What is machine learning?")

# Multiple documents
doc_embeddings = embeddings.embed_documents([
    "Document 1 text",
    "Document 2 text",
    "Document 3 text"
])
```

**Available Embedding Models:**
- **Amazon Titan**: 
  - `amazon.titan-embed-text-v1` - 1,536 dimensions
  - `amazon.titan-embed-text-v2:0` - 1,024 dimensions (default)
- **Cohere**:
  - `cohere.embed-english-v3` - 1,024 dimensions
  - `cohere.embed-multilingual-v3` - 1,024 dimensions

**Configuration Options:**
```python
embeddings = BedrockEmbeddings(
    model_id="amazon.titan-embed-text-v2:0",
    model_kwargs={
        "inputText": "",  # Will be set automatically
        "dimensions": 1024,
        "normalize": True
    },
    region_name="us-east-1"
)
```

### Retrievers

#### AmazonKendraRetriever
Retriever for Amazon Kendra enterprise search service.

```python
from langchain_aws import AmazonKendraRetriever

retriever = AmazonKendraRetriever(
    index_id="your-kendra-index-id",
    region_name="us-east-1",
    top_k=5,
    attribute_filter={
        "EqualsTo": {
            "Key": "_language_code",
            "Value": {"StringValue": "en"}
        }
    }
)

docs = retriever.get_relevant_documents("What is machine learning?")
```

**Key Features:**
- Natural language querying
- Faceted search and filtering
- Document ranking and scoring
- Support for multiple document formats (PDF, HTML, Word, PowerPoint, etc.)
- Enterprise connectors (SharePoint, S3, databases, etc.)
- Access control and security filtering

**Configuration Parameters:**
```python
retriever = AmazonKendraRetriever(
    index_id="your-index-id",
    top_k=10,
    attribute_filter={
        "AndAllFilters": [
            {
                "EqualsTo": {
                    "Key": "_category",
                    "Value": {"StringValue": "IT"}
                }
            },
            {
                "DateRange": {
                    "Key": "_last_updated_at",
                    "Value": {
                        "GreaterThan": {"DateValue": datetime(2023, 1, 1)}
                    }
                }
            }
        ]
    },
    user_context={
        "UserId": "user123",
        "Groups": ["IT", "Engineering"]
    }
)
```

#### AmazonKnowledgeBasesRetriever
Retriever for Amazon Bedrock Knowledge Bases.

```python
from langchain_aws import AmazonKnowledgeBasesRetriever

retriever = AmazonKnowledgeBasesRetriever(
    knowledge_base_id="YOUR_KNOWLEDGE_BASE_ID",
    retrieval_config={
        "vectorSearchConfiguration": {
            "numberOfResults": 4,
            "overrideSearchType": "HYBRID",
            "filter": {
                "equals": {
                    "key": "category",
                    "value": "science"
                }
            }
        }
    },
    region_name="us-east-1"
)

docs = retriever.get_relevant_documents("Explain quantum computing")
```

**Features:**
- Vector search with semantic similarity
- Hybrid search (vector + keyword)
- Metadata filtering
- Integration with Bedrock foundation models
- Automatic chunking and indexing
- Real-time updates

**Search Types:**
- `VECTOR`: Pure vector similarity search
- `HYBRID`: Combines vector and keyword search
- Default is determined by knowledge base configuration

### Graph Databases

#### NeptuneGraph
Interface for Amazon Neptune graph database (property graphs).

```python
from langchain_aws import NeptuneGraph

graph = NeptuneGraph(
    host="your-neptune-cluster.cluster-xyz.us-east-1.neptune.amazonaws.com",
    port=8182,
    use_https=True,
    region_name="us-east-1"
)

# Execute Gremlin query
result = graph.query("g.V().limit(10)")

# With parameters
result = graph.query(
    "g.V().has('name', name).out('knows').values('name')",
    params={"name": "Alice"}
)
```

**Supported Features:**
- Gremlin query language
- Property graph model
- Graph traversals and analytics
- ACID transactions
- Read replicas for scaling
- Multi-AZ deployment

#### NeptuneAnalyticsGraph
Interface for Amazon Neptune Analytics (serverless graph analytics).

```python
from langchain_aws import NeptuneAnalyticsGraph

graph = NeptuneAnalyticsGraph(
    graph_identifier="your-graph-id",
    region_name="us-east-1"
)

# Execute analytics query
result = graph.query("MATCH (n) RETURN count(n)")
```

**Features:**
- Serverless graph analytics
- Fast graph queries and algorithms
- Built-in graph algorithms (centrality, community detection, etc.)
- Pay-per-query pricing
- Automatic scaling

## AWS Services Integration

### Amazon Bedrock

Amazon Bedrock provides access to foundation models from leading AI companies through a single API.

**Key Capabilities:**
- **Model Choice**: Access to multiple foundation model providers
- **Serverless Experience**: No infrastructure management required
- **Responsible AI**: Built-in content filtering and safety features
- **Private Customization**: Fine-tune models with your own data
- **Enterprise Ready**: VPC support, encryption, and compliance

**Model Families Available:**
1. **Text Generation**: Claude, Jurassic-2, Command, Llama 2, Titan Text
2. **Embeddings**: Titan Embeddings, Cohere Embeddings
3. **Image Generation**: Stable Diffusion XL, Titan Image Generator
4. **Code Generation**: CodeWhisperer integration

### Amazon SageMaker

Amazon SageMaker is a fully managed service for building, training, and deploying machine learning models.

**Integration Features:**
- **Custom Model Endpoints**: Deploy any ML model for real-time inference
- **Batch Transform**: Process large datasets with batch inference
- **Multi-Model Endpoints**: Host multiple models on a single endpoint
- **Auto Scaling**: Automatic scaling based on traffic
- **A/B Testing**: Traffic splitting for model comparison

**Supported Frameworks:**
- PyTorch, TensorFlow, Scikit-learn, XGBoost, Hugging Face
- Custom containers with Docker
- Pre-built containers for popular frameworks

### Amazon Kendra

Amazon Kendra is an enterprise search service powered by machine learning.

**Search Capabilities:**
- **Natural Language**: Query using natural language questions
- **Document Understanding**: Automatic extraction of entities and relationships
- **Incremental Learning**: Improves results based on user interactions
- **Enterprise Connectors**: 50+ connectors for data sources
- **Custom Document Types**: Support for custom document formats

**Data Sources:**
- File systems (S3, SharePoint, OneDrive)
- Databases (RDS, DynamoDB, etc.)
- SaaS applications (Salesforce, ServiceNow, etc.)
- Websites and web crawlers
- Custom data sources via API

### Amazon Neptune

Amazon Neptune is a fully managed graph database service.

**Graph Features:**
- **Property Graphs**: Using Apache TinkerPop Gremlin
- **RDF Graphs**: Using SPARQL query language
- **High Availability**: Multi-AZ deployment with automatic failover
- **Read Replicas**: Up to 15 read replicas for scaling reads
- **Point-in-Time Recovery**: Continuous backup to S3
- **Encryption**: At-rest and in-transit encryption

**Use Cases:**
- Knowledge graphs and semantic search
- Fraud detection and identity graphs
- Recommendation engines
- Network and IT operations
- Social networking applications

## Authentication and Configuration

### AWS Credentials Configuration

The package uses boto3 for AWS service interactions. Configure credentials using any of these methods:

1. **AWS Credentials File** (`~/.aws/credentials`):
```ini
[default]
aws_access_key_id = YOUR_ACCESS_KEY
aws_secret_access_key = YOUR_SECRET_KEY
region = us-east-1
```

2. **Environment Variables**:
```bash
export AWS_ACCESS_KEY_ID=your_access_key
export AWS_SECRET_ACCESS_KEY=your_secret_key
export AWS_DEFAULT_REGION=us-east-1
```

3. **IAM Roles** (for EC2 instances):
Attach an IAM role with appropriate permissions to your EC2 instance.

4. **AWS CLI Configuration**:
```bash
aws configure
```

### Region Configuration

Specify AWS regions for services:

```python
import os
os.environ["AWS_DEFAULT_REGION"] = "us-west-2"

# Or via parameter
llm = ChatBedrock(
    model_id="anthropic.claude-v2:1",
    region_name="us-west-2"
)
```

### Required IAM Permissions

**For Bedrock:**
```json
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "bedrock:InvokeModel",
                "bedrock:InvokeModelWithResponseStream"
            ],
            "Resource": "*"
        }
    ]
}
```

**For Kendra:**
```json
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "kendra:Query",
                "kendra:Retrieve"
            ],
            "Resource": "arn:aws:kendra:region:account:index/index-id"
        }
    ]
}
```

**For SageMaker:**
```json
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "sagemaker:InvokeEndpoint"
            ],
            "Resource": "arn:aws:sagemaker:region:account:endpoint/endpoint-name"
        }
    ]
}
```

## Advanced Usage Patterns

### RAG with Bedrock and Kendra

```python
from langchain_aws import ChatBedrock, AmazonKendraRetriever
from langchain.chains import RetrievalQA

# Setup retriever
retriever = AmazonKendraRetriever(
    index_id="your-kendra-index",
    top_k=5
)

# Setup LLM
llm = ChatBedrock(
    model_id="anthropic.claude-3-sonnet-20240229-v1:0",
    model_kwargs={"max_tokens": 1000}
)

# Create RAG chain
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=retriever,
    return_source_documents=True
)

# Query
result = qa_chain({"query": "What are the company's data privacy policies?"})
```

### Multi-Model Bedrock Pipeline

```python
from langchain_aws import ChatBedrock, BedrockEmbeddings
from langchain.schema import HumanMessage

# Different models for different tasks
embeddings_model = BedrockEmbeddings(model_id="amazon.titan-embed-text-v1")
reasoning_model = ChatBedrock(model_id="anthropic.claude-3-opus-20240229-v1:0")
speed_model = ChatBedrock(model_id="anthropic.claude-3-haiku-20240307-v1:0")

# Use speed model for initial processing
quick_response = speed_model.invoke([
    HumanMessage(content="Summarize this document in one sentence: " + document)
])

# Use reasoning model for complex analysis
detailed_analysis = reasoning_model.invoke([
    HumanMessage(content="Analyze the implications: " + quick_response.content)
])
```

### Custom SageMaker Integration

```python
from langchain_aws.llms.sagemaker_endpoint import LLMContentHandler
import json

class HuggingFaceContentHandler(LLMContentHandler):
    content_type = "application/json"
    accepts = "application/json"
    
    def transform_input(self, prompt: str, model_kwargs: dict) -> bytes:
        payload = {
            "inputs": prompt,
            "parameters": {
                "max_length": model_kwargs.get("max_length", 100),
                "temperature": model_kwargs.get("temperature", 0.7),
                "do_sample": model_kwargs.get("do_sample", True)
            }
        }
        return json.dumps(payload).encode('utf-8')
    
    def transform_output(self, output: bytes) -> str:
        response = json.loads(output.read().decode("utf-8"))
        return response[0]["generated_text"]

# Deploy custom model
llm = SagemakerEndpoint(
    endpoint_name="huggingface-pytorch-tgi-inference-2023-09-25-19-53-32-140",
    region_name="us-east-1",
    content_handler=HuggingFaceContentHandler(),
    model_kwargs={"max_length": 200, "temperature": 0.8}
)
```

## Performance Optimization

### Bedrock Optimization

```python
# Use streaming for long responses
llm = ChatBedrock(
    model_id="anthropic.claude-3-sonnet-20240229-v1:0",
    streaming=True,
    model_kwargs={
        "max_tokens": 4000,
        "temperature": 0.7
    }
)

# Process streaming response
for chunk in llm.stream("Write a long article about AI"):
    print(chunk.content, end="", flush=True)
```

### Embedding Batch Processing

```python
from langchain_aws import BedrockEmbeddings

embeddings = BedrockEmbeddings(model_id="amazon.titan-embed-text-v1")

# Process documents in batches
documents = ["doc1", "doc2", "doc3", ...]  # Large list
batch_size = 25  # Bedrock limit

all_embeddings = []
for i in range(0, len(documents), batch_size):
    batch = documents[i:i + batch_size]
    batch_embeddings = embeddings.embed_documents(batch)
    all_embeddings.extend(batch_embeddings)
```

### Connection Pooling and Caching

```python
import boto3
from functools import lru_cache

# Reuse boto3 sessions
session = boto3.Session(region_name='us-east-1')

@lru_cache(maxsize=100)
def get_embedding(text: str):
    embeddings = BedrockEmbeddings(
        model_id="amazon.titan-embed-text-v1",
        client=session.client('bedrock-runtime')
    )
    return embeddings.embed_query(text)
```

## Error Handling and Resilience

### Common Error Patterns

```python
from botocore.exceptions import ClientError
from langchain_aws import ChatBedrock
import time
import random

def bedrock_with_retry(llm, prompt, max_retries=3):
    for attempt in range(max_retries):
        try:
            return llm.invoke(prompt)
        except ClientError as e:
            error_code = e.response['Error']['Code']
            
            if error_code == 'ThrottlingException':
                # Exponential backoff
                wait_time = (2 ** attempt) + random.uniform(0, 1)
                time.sleep(wait_time)
                continue
            elif error_code == 'ValidationException':
                raise ValueError(f"Invalid request: {e}")
            elif error_code == 'AccessDeniedException':
                raise PermissionError("Check your AWS permissions")
            else:
                raise e
    
    raise Exception(f"Failed after {max_retries} attempts")

# Usage
llm = ChatBedrock(model_id="anthropic.claude-v2:1")
response = bedrock_with_retry(llm, "Hello, world!")
```

### Kendra Error Handling

```python
from langchain_aws import AmazonKendraRetriever

def safe_kendra_query(retriever, query, fallback_docs=None):
    try:
        return retriever.get_relevant_documents(query)
    except ClientError as e:
        error_code = e.response['Error']['Code']
        
        if error_code == 'ResourceNotFoundException':
            print("Kendra index not found")
            return fallback_docs or []
        elif error_code == 'ThrottlingException':
            print("Kendra rate limit exceeded")
            time.sleep(1)
            return retriever.get_relevant_documents(query)
        else:
            print(f"Kendra error: {e}")
            return fallback_docs or []
```

## Security Best Practices

### Input Sanitization

```python
import re
from langchain_aws import ChatBedrock

def sanitize_input(prompt: str) -> str:
    # Remove potential prompt injection attempts
    dangerous_patterns = [
        r'ignore previous instructions',
        r'system[:\s]*',
        r'assistant[:\s]*',
        r'human[:\s]*'
    ]
    
    for pattern in dangerous_patterns:
        prompt = re.sub(pattern, '', prompt, flags=re.IGNORECASE)
    
    return prompt[:4000]  # Limit length

def safe_llm_call(llm, user_input: str):
    sanitized_input = sanitize_input(user_input)
    return llm.invoke(sanitized_input)
```

### VPC Configuration

```python
# Configure VPC endpoints for private connectivity
import boto3

session = boto3.Session()
bedrock_client = session.client(
    'bedrock-runtime',
    region_name='us-east-1',
    config=boto3.Config(
        # Use VPC endpoint
        endpoint_url='https://vpce-xyz.bedrock-runtime.us-east-1.vpce.amazonaws.com'
    )
)

llm = ChatBedrock(
    model_id="anthropic.claude-v2:1",
    client=bedrock_client
)
```

## Cost Management

### Token Usage Monitoring

```python
from langchain_aws import ChatBedrock
from langchain.callbacks import get_openai_callback

# For Bedrock (approximate monitoring)
class BedrockTokenCounter:
    def __init__(self):
        self.total_tokens = 0
    
    def estimate_tokens(self, text: str) -> int:
        # Rough estimation: 1 token ≈ 4 characters
        return len(text) // 4
    
    def count_llm_call(self, prompt: str, response: str):
        input_tokens = self.estimate_tokens(prompt)
        output_tokens = self.estimate_tokens(response)
        self.total_tokens += input_tokens + output_tokens
        
        print(f"Estimated tokens - Input: {input_tokens}, Output: {output_tokens}")
        print(f"Total tokens used: {self.total_tokens}")

counter = BedrockTokenCounter()
llm = ChatBedrock(model_id="anthropic.claude-v2:1")

prompt = "Explain machine learning"
response = llm.invoke(prompt)
counter.count_llm_call(prompt, response.content)
```

### Batch Processing for Efficiency

```python
from langchain_aws import BedrockEmbeddings
from typing import List

def efficient_embedding_processing(texts: List[str]) -> List[List[float]]:
    embeddings = BedrockEmbeddings(model_id="amazon.titan-embed-text-v1")
    
    # Process in optimal batches
    batch_size = 25  # Bedrock's batch limit
    all_embeddings = []
    
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i + batch_size]
        print(f"Processing batch {i//batch_size + 1}/{(len(texts)-1)//batch_size + 1}")
        
        batch_embeddings = embeddings.embed_documents(batch)
        all_embeddings.extend(batch_embeddings)
    
    return all_embeddings
```

## Migration Guide

### From langchain-community

```python
# OLD - langchain-community imports
from langchain_community.llms import Bedrock
from langchain_community.embeddings import BedrockEmbeddings
from langchain_community.retrievers import AmazonKendraRetriever

# NEW - langchain-aws imports
from langchain_aws import BedrockLLM, ChatBedrock
from langchain_aws import BedrockEmbeddings  
from langchain_aws import AmazonKendraRetriever

# Class name changes
old_llm = Bedrock(model_id="anthropic.claude-v2:1")
new_llm = BedrockLLM(model_id="anthropic.claude-v2:1")  # Same parameters
new_chat = ChatBedrock(model_id="anthropic.claude-v2:1")  # Recommended

# Embeddings and retrievers use same class names
embeddings = BedrockEmbeddings(model_id="amazon.titan-embed-text-v1")
retriever = AmazonKendraRetriever(index_id="your-index-id")
```

### Parameter Mapping

```python
# Old Bedrock LLM parameters
old_params = {
    "model_id": "anthropic.claude-v2:1",
    "model_kwargs": {
        "max_tokens_to_sample": 1000,
        "temperature": 0.7,
        "top_p": 0.9,
        "stop_sequences": ["\n\nHuman:"]
    }
}

# New ChatBedrock parameters (recommended)
new_params = {
    "model_id": "anthropic.claude-v2:1",
    "model_kwargs": {
        "max_tokens": 1000,  # Note: parameter name changed
        "temperature": 0.7,
        "top_p": 0.9,
        "stop_sequences": ["\n\nHuman:"]
    }
}
```

## Testing and Development

### Unit Testing

```python
import pytest
from unittest.mock import MagicMock, patch
from langchain_aws import ChatBedrock

@patch('langchain_aws.chat_models.bedrock.boto3')
def test_bedrock_chat(mock_boto3):
    # Mock the Bedrock client
    mock_client = MagicMock()
    mock_boto3.client.return_value = mock_client
    
    # Mock response
    mock_response = {
        'body': MagicMock()
    }
    mock_response['body'].read.return_value = b'{"completion": "Hello!"}'
    mock_client.invoke_model.return_value = mock_response
    
    # Test the integration
    llm = ChatBedrock(model_id="anthropic.claude-v2:1")
    response = llm.invoke("Hi there")
    
    assert "Hello!" in response.content
    mock_client.invoke_model.assert_called_once()
```

### Integration Testing

```python
import os
import pytest
from langchain_aws import ChatBedrock, BedrockEmbeddings

@pytest.mark.integration
@pytest.mark.skipif(not os.getenv("AWS_ACCESS_KEY_ID"), reason="AWS credentials required")
def test_bedrock_integration():
    llm = ChatBedrock(model_id="anthropic.claude-3-haiku-20240307-v1:0")
    response = llm.invoke("What is 2+2?")
    
    assert response.content
    assert "4" in response.content

@pytest.mark.integration
def test_embeddings_integration():
    embeddings = BedrockEmbeddings(model_id="amazon.titan-embed-text-v1")
    
    # Test single embedding
    result = embeddings.embed_query("test query")
    assert len(result) == 1536  # Titan v1 dimensions
    
    # Test batch embeddings
    results = embeddings.embed_documents(["doc1", "doc2"])
    assert len(results) == 2
    assert all(len(emb) == 1536 for emb in results)
```

## Troubleshooting

### Common Issues and Solutions

1. **Model Not Found Error**:
```python
# Error: Could not resolve model
# Solution: Check model availability in your region
available_models = {
    "us-east-1": ["anthropic.claude-v2:1", "anthropic.claude-3-sonnet-20240229-v1:0"],
    "us-west-2": ["anthropic.claude-v2:1", "amazon.titan-text-express-v1"]
}
```

2. **Rate Limiting**:
```python
# Implement exponential backoff
import time
from botocore.exceptions import ClientError

def invoke_with_backoff(llm, prompt, max_retries=5):
    for i in range(max_retries):
        try:
            return llm.invoke(prompt)
        except ClientError as e:
            if e.response['Error']['Code'] == 'ThrottlingException':
                wait_time = (2 ** i) + random.uniform(0, 1)
                time.sleep(wait_time)
            else:
                raise
    raise Exception("Max retries exceeded")
```

3. **Memory Issues with Large Batches**:
```python
# Process embeddings in smaller chunks
def process_large_document_set(documents, batch_size=10):
    embeddings = BedrockEmbeddings(model_id="amazon.titan-embed-text-v1")
    results = []
    
    for i in range(0, len(documents), batch_size):
        batch = documents[i:i + batch_size]
        batch_results = embeddings.embed_documents(batch)
        results.extend(batch_results)
        
        # Optional: Clear memory
        if i % 100 == 0:
            import gc
            gc.collect()
    
    return results
```

## Version Compatibility

- **Python**: 3.8.1 or higher
- **LangChain Core**: 0.2.6 or higher  
- **Boto3**: 1.34.131 to 1.35.0
- **NumPy**: 1.0 or higher

## Contributing

### Development Setup

```bash
# Clone the repository
git clone https://github.com/langchain-ai/langchain-aws.git
cd langchain-aws/libs/aws

# Install Poetry (if not already installed)
curl -sSL https://install.python-poetry.org | python3 -

# Install dependencies
poetry install --with lint,typing,test,test_integration,dev

# Activate virtual environment
poetry shell

# Run tests
make test

# Run integration tests (requires AWS credentials)
make integration_test

# Format code
make format

# Lint code
make lint
```

### Code Standards

- Follow PEP 8 style guidelines
- Use type hints for all function parameters and return values
- Add docstrings for all public methods and classes
- Write unit tests for new functionality
- Add integration tests for AWS service interactions

### Pull Request Process

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests
5. Run `make format` and `make lint`
6. Submit a pull request with a clear description

## Resources

- **GitHub Repository**: https://github.com/langchain-ai/langchain-aws
- **PyPI Package**: https://pypi.org/project/langchain-aws/
- **LangChain Documentation**: https://python.langchain.com/docs/integrations/platforms/aws/
- **AWS Bedrock Documentation**: https://docs.aws.amazon.com/bedrock/
- **Amazon SageMaker Documentation**: https://docs.aws.amazon.com/sagemaker/
- **Amazon Kendra Documentation**: https://docs.aws.amazon.com/kendra/
- **Amazon Neptune Documentation**: https://docs.aws.amazon.com/neptune/

## Support

For issues and questions:
- **GitHub Issues**: https://github.com/langchain-ai/langchain-aws/issues
- **LangChain Community**: https://github.com/langchain-ai/langchain/discussions  
- **AWS Support**: https://aws.amazon.com/support/

## License

This project is licensed under the MIT License. See the LICENSE file for details.