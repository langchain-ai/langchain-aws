{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bedrock AgentCore Memory Store Walkthrough - Long Term Memory\n",
    "\n",
    "This sample notebook walks through setup and usage of the Bedrock AgentCore Memory Store with LangGraph. This approach enables saving of conversations to the AgentCore memory API to be later extracted and retrieved, enabling long term memory.\n",
    "\n",
    "### Setup\n",
    "For this notebook you will need:\n",
    "1. An Amazon Web Services development account\n",
    "2. Bedrock Model Access (i.e. Claude 3.7 Sonnet)\n",
    "3. An AgentCore Memory Resource configured (see below section for details)\n",
    "4. Two strategies enabled for the Agent Core Memory resource, `/facts/{actor_id}` semantic search and `/preferences/{actor_id}` user preference search\n",
    "\n",
    "### AgentCore Memory Resource\n",
    "\n",
    "Either in the AWS developer portal or using the boto3 library you must create an [AgentCore Memory Resource](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock-agentcore-control/client/create_memory.html). For this notebook, only two strategies need to be enabled, user preferences and semantic memory. These strategies will automatically run once we save our conversational messages to AgentCore Memory and extract chunks of information that our agent can retrieve later. For more information on long term memory, see the docs here [AgentCore Long Term Memory](https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/long-term-memory.html).\n",
    "\n",
    "Once you have the Memory enabled and in a `ACTIVE` state, take note of the `memoryId` and strategy namespaces, we will need them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain langchain-aws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import LangGraph and LangChain components\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langgraph.store.base import BaseStore\n",
    "import uuid\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the AgentCoreMemoryStore that we will use as a store\n",
    "from langgraph_checkpoint_aws import (\n",
    "    AgentCoreMemoryStore\n",
    ")\n",
    "\n",
    "# For this example, we will just use an InMemorySaver to save context.\n",
    "# In production, we highly recommend the AgentCoreMemorySaver as a checkpointer\n",
    "# which works seamlessly alongside the memory store\n",
    "from langgraph.checkpoint.memory import InMemorySaver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AgentCore Memory Configuration\n",
    "- `REGION` corresponds to the AWS region that your resources are present in, these are passed to the `AgentCoreMemorySaver`.\n",
    "- `MEMORY_ID` corresponds to your top level AgentCore Memory resource. Within this resource we will store checkpoints for multiple actors and sessions\n",
    "- `MODEL_ID` this is the bedrock model that will power our LangGraph agent through Bedrock Converse.\n",
    "\n",
    "We will use the `MEMORY_ID` and any additional boto3 client keyword args (in our case, `REGION`) to instantiate our checkpointer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = \"us-west-2\"\n",
    "MEMORY_ID = \"YOUR_MEMORY_ID\"\n",
    "MODEL_ID = \"us.anthropic.claude-3-7-sonnet-20250219-v1:0\"\n",
    "\n",
    "# Initialize the store to enable long term memory saving and retrieval\n",
    "store = AgentCoreMemoryStore(memory_id=MEMORY_ID, region_name=REGION)\n",
    "\n",
    "# Initialize Bedrock LLM\n",
    "llm = init_chat_model(MODEL_ID, model_provider=\"bedrock_converse\", region_name=REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing the messages that we want for long term memory extraction\n",
    "\n",
    "In Bedrock AgentCore, each conversational message that is added for an `actor_id` and `session_id` is processed in the service according to the strategies specified for that memory resource. In our case, since we defined two strategies, user preference and semantic extraction, both user preferences and facts from previous conversations will be stored as long term memories in the service that can be retrieved by our retrieval tool above.\n",
    "\n",
    "For this example, we will use the AgentCoreMemoryStore to save messages in the pre and post model hooks so that we capture both the HumanMessage objects and the AIMessage objects that occur during the conversation. The pre-model hook runs before the LLM invocation and the post-model hook runs after it to capture the response from the agent.\n",
    "\n",
    "The pre-model hook will also perform a semantic search using the user's latest message to add a few relevant results to the LLMs input so it can have context into the users past preferences. If needed, the LLM can also search itself using the semantic search tool.\n",
    "\n",
    "**Note**: LangChain message types are converted under the hood by the store to AgentCore Memory message types so that they can be properly extracted to long term memories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_model_hook(state, config: RunnableConfig, *, store: BaseStore):\n",
    "    \"\"\"Hook that runs pre-LLM invocation to save the latest human message\"\"\"\n",
    "    actor_id = config[\"configurable\"][\"actor_id\"]\n",
    "    thread_id = config[\"configurable\"][\"thread_id\"]\n",
    "    # Saving the message to the actor and session combination that we get at runtime\n",
    "    namespace = (actor_id, thread_id)\n",
    "    \n",
    "    messages = state.get(\"messages\", [])\n",
    "    # Save the last human message we see before LLM invocation\n",
    "    for msg in reversed(messages):\n",
    "        if isinstance(msg, HumanMessage):\n",
    "            store.put(namespace, str(uuid.uuid4()), {\"message\": msg})\n",
    "            break\n",
    "    # Retrieve user preferences based on the last message and append to state\n",
    "    user_preferences_namespace = (\"preferences\", actor_id)\n",
    "    preferences = store.search(user_preferences_namespace, query=msg.content, limit=5)\n",
    "    \n",
    "    # Construct another AI message to add context before the current message\n",
    "    if preferences:\n",
    "        context_items = [pref.value for pref in preferences]\n",
    "        context_message = AIMessage(\n",
    "            content=f\"[User Context: {', '.join(str(item) for item in context_items)}]\"\n",
    "        )\n",
    "        # Insert the context message before the last human message\n",
    "        return {\"messages\": messages[:-1] + [context_message, messages[-1]]}\n",
    "    \n",
    "    return {\"llm_input_messages\": messages}\n",
    "\n",
    "def post_model_hook(state, config: RunnableConfig, *, store: BaseStore):\n",
    "    \"\"\"Hook that runs post-LLM invocation to save the latest human message\"\"\"\n",
    "    actor_id = config[\"configurable\"][\"actor_id\"]\n",
    "    thread_id = config[\"configurable\"][\"thread_id\"]\n",
    "\n",
    "    # Saving the message to the actor and session combination that we get at runtime\n",
    "    namespace = (actor_id, thread_id)\n",
    "    \n",
    "    messages = state.get(\"messages\", [])\n",
    "    # Save the LLMs response to AgentCore Memory\n",
    "    for msg in reversed(messages):\n",
    "        if isinstance(msg, AIMessage):\n",
    "            store.put(namespace, str(uuid.uuid4()), {\"message\": msg})\n",
    "            break\n",
    "    \n",
    "    return {\"messages\": messages}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build our LangGraph agent graph\n",
    "\n",
    "Our agent will be built with the `create_react_agent` builder. It just has a few simple nodes, mainly a chatbot node and a tool node. The tool node will contain just our long term memory retrieval tool and the pre and post model hooks are specified as arguments.\n",
    "\n",
    "**Note**: for custom agent implementations the Store and tools can be configured to run as needed for any workflow following this pattern. Pre/post model hooks can be used, the whole conversation could be saved at the end, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = create_react_agent(\n",
    "    llm,\n",
    "    store=store,\n",
    "    tools=[],\n",
    "    checkpointer=InMemorySaver(),\n",
    "    pre_model_hook=pre_model_hook,\n",
    "    post_model_hook=post_model_hook\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTANT: Input and Config\n",
    "\n",
    "### Graph Invoke Input\n",
    "We only need to pass the newest user message in as an argument `inputs`. This could include other state variables too but for the simple `create_react_agent`, messages are all that's required.\n",
    "\n",
    "### LangGraph RuntimeConfig\n",
    "In LangGraph, config is a `RuntimeConfig` that contains attributes that are necessary at invocation time, for example user IDs or session IDs. For the `AgentCoreMemorySaver`, `thread_id` and `actor_id` must be set in the config. For instance, your AgentCore invocation endpoint could assign this based on the identity or user ID of the caller. Additional documentation here: [https://langchain-ai.github.io/langgraphjs/how-tos/configuration/](https://langchain-ai.github.io/langgraphjs/how-tos/configuration/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"configurable\": {\n",
    "        \"thread_id\": \"session-id-1\", # REQUIRED: This maps to Bedrock AgentCore session_id under the hood\n",
    "        \"actor_id\": \"user-id-1\", # REQUIRED: This maps to Bedrock AgentCore actor_id under the hood\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Agent\n",
    "\n",
    "For this example, we will run through a conversation where the user is talking about what they like to cook with. This will give the backend enough context to extract facts and user preferences that we can retrieve the next time the user asks for what to make on a given evening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "\n",
      "Hey there! Im cooking one of my favorite meals tonight, salmon with rice and veggies (healthy). Has\n",
      "great macros for my weightlifting competition that is coming up. What can I add to this dish to make it taste better\n",
      "and also improve the protein and vitamins I get?\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "# Enhancing Your Salmon, Rice and Veggie Meal\n",
      "\n",
      "Great choice for your weightlifting competition prep! Here are some flavorful additions that will boost both taste and nutrition:\n",
      "\n",
      "## For Extra Protein\n",
      "- **Greek yogurt sauce**: Mix with lemon, dill and garlic for a protein-rich topping\n",
      "- **Sprinkle of hemp seeds**: Adds 10g protein per 3 tablespoons\n",
      "- **Quinoa**: Mix with or replace some rice for complete protein\n",
      "- **Edamame**: Mix into your rice for plant-based protein\n",
      "\n",
      "## For More Vitamins & Minerals\n",
      "- **Spinach or kale**: Saut√© with your other veggies for iron and vitamin K\n",
      "- **Avocado slices**: Adds healthy fats and vitamin E\n",
      "- **Roasted bell peppers**: For vitamin C and color\n",
      "- **Fresh herbs**: Cilantro, basil or parsley add micronutrients and flavor\n",
      "\n",
      "## Flavor Enhancers (without compromising nutrition)\n",
      "- **Citrus zest**: Lemon or lime zest brightens without calories\n",
      "- **Miso glaze**: For umami flavor and probiotics\n",
      "- **Turmeric and black pepper**: Anti-inflammatory benefits\n",
      "- **Toasted sesame oil**: Just a drizzle adds significant flavor\n",
      "\n",
      "Would you like specific prep methods for any of these additions?\n"
     ]
    }
   ],
   "source": [
    "# Helper function to pretty print agent output while running\n",
    "def run_agent(query: str, config: RunnableConfig):\n",
    "    printed_ids = set()\n",
    "    events = graph.stream(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n",
    "        config,\n",
    "        stream_mode=\"values\",\n",
    "    )\n",
    "    for event in events:\n",
    "        if \"messages\" in event:\n",
    "            for msg in event[\"messages\"]:\n",
    "                # Check if we've already printed this message\n",
    "                if id(msg) not in printed_ids:\n",
    "                    msg.pretty_print()\n",
    "                    printed_ids.add(id(msg))\n",
    "\n",
    "\n",
    "prompt = \"\"\"\n",
    "Hey there! Im cooking one of my favorite meals tonight, salmon with rice and veggies (healthy). Has\n",
    "great macros for my weightlifting competition that is coming up. What can I add to this dish to make it taste better\n",
    "and also improve the protein and vitamins I get?\n",
    "\"\"\"\n",
    "\n",
    "run_agent(prompt, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What was stored?\n",
    "As you can see, the model does not yet have any insight into our preferences or dietary restrictions.\n",
    "\n",
    "For this implementation with pre/post model hooks, two messages were stored here. The first message from the user and the response from the AI model were both stored as conversational events in AgentCore Memory.\n",
    "\n",
    "These messages were then extracted to AgentCore long term memory in our fact and user preferences namespaces. In fact, we can check the store ourselves to verify what has been stored there so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Facts namespace result: [Item(namespace=['facts', 'user-1'], key='mem-596ad8e5-f561-4bea-861a-c498276e92f8', value={'content': 'The user is preparing for a weightlifting competition and is cooking a healthy meal of salmon, rice, and vegetables.', 'memory_strategy_id': 'memory_semantic_ghc4p-SLjZ3l87ji', 'namespaces': ['/facts/user-1']}, created_at='2025-09-24T17:55:09-07:00', updated_at='2025-09-24T17:55:09-07:00', score=0.40105253), Item(namespace=['facts', 'user-1'], key='mem-3a941844-d352-4f5b-af44-f2e2353f75cb', value={'content': 'The user is preparing for a weightlifting competition and prioritizes nutritionally balanced meals with good macronutrients, such as salmon with rice and vegetables.', 'memory_strategy_id': 'memory_semantic_ghc4p-SLjZ3l87ji', 'namespaces': ['/facts/user-12345']}, created_at='2025-09-25T21:38:33-07:00', updated_at='2025-09-25T21:38:33-07:00', score=0.39868504), Item(namespace=['facts', 'user-1'], key='mem-c040719d-81a4-41eb-9b8a-6d4848d75dcf', value={'content': 'The user is focused on maintaining good macronutrient balance in their diet to support their weightlifting training.', 'memory_strategy_id': 'memory_semantic_ghc4p-SLjZ3l87ji', 'namespaces': ['/facts/user-1']}, created_at='2025-09-24T17:55:09-07:00', updated_at='2025-09-24T17:55:09-07:00', score=0.37003443)]\n",
      "\n",
      "Preferences namespace result: [Item(namespace=['preferences', 'user-1'], key='mem-52e75280-1c1c-41aa-874b-e4725dba60bc', value={'content': '{\"context\":\"User mentioned cooking salmon with rice and vegetables, indicating a focus on healthy eating for a weightlifting competition\",\"preference\":\"Follows a health-conscious diet with high-protein, nutritious meals\",\"categories\":[\"food\",\"nutrition\",\"fitness\",\"diet\"]}', 'memory_strategy_id': 'memory_preference_ghc4p-zjtCX1D9Eo', 'namespaces': ['/preferences/user-1']}, created_at='2025-09-24T17:55:09-07:00', updated_at='2025-09-24T17:55:09-07:00', score=0.40248412), Item(namespace=['preferences', 'user-1'], key='mem-59336315-4723-4277-9f22-acbc3376f43f', value={'content': '{\"context\":\"User mentioned cooking salmon with rice and veggies, and preparing for a weightlifting competition\",\"preference\":\"Interested in healthy, nutritionally balanced meals that support athletic performance\",\"categories\":[\"food\",\"nutrition\",\"fitness\"]}', 'memory_strategy_id': 'memory_preference_ghc4p-zjtCX1D9Eo', 'namespaces': ['/preferences/user-12345']}, created_at='2025-09-25T21:36:58-07:00', updated_at='2025-09-25T21:36:58-07:00', score=0.39147264), Item(namespace=['preferences', 'user-1'], key='mem-4e4031c9-4629-4aef-b8b5-09a48a252c8e', value={'content': '{\"context\":\"User is preparing for a weightlifting competition and is concerned about meal\\'s nutritional value\",\"preference\":\"Interested in meals that support athletic performance and nutrition\",\"categories\":[\"fitness\",\"nutrition\",\"sports\"]}', 'memory_strategy_id': 'memory_preference_ghc4p-zjtCX1D9Eo', 'namespaces': ['/preferences/user-1']}, created_at='2025-09-24T17:55:09-07:00', updated_at='2025-09-24T17:55:09-07:00', score=0.38236186)]\n"
     ]
    }
   ],
   "source": [
    "# Search our facts namespace (the one that is used by the agent long term retrieval)\n",
    "search_namespace = (\"facts\", \"user-1\")\n",
    "result = store.search(search_namespace, query=\"food\", limit=3)\n",
    "print(f\"Facts namespace result: {result}\\n\")\n",
    "\n",
    "# Search our user preferences namespace\n",
    "search_namespace = (\"preferences\", \"user-1\")\n",
    "result = store.search(search_namespace, query=\"food\", limit=3)\n",
    "print(f\"Preferences namespace result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent access to the store\n",
    "\n",
    "**Note** - since AgentCore memory processes these events in the background, it may take a few seconds for the memory to be extracted and embedded to long term memory retrieval.\n",
    "\n",
    "Great! Now we have seen that long term memories were extracted to our namespaces based on the earlier messages in the conversation.\n",
    "\n",
    "Now, let's start a new session and ask about recommendations for what to cook for dinner. The agent can use the store to access the long term memories that were extracted to make a recommendation that the user will be sure to like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Today's a new day, what should I make for dinner tonight?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "[User Context: {'content': '{\"context\":\"User specifically mentioned cooking salmon, rice, and vegetables for a meal\",\"preference\":\"Enjoys healthy, protein-rich meals\",\"categories\":[\"food\",\"cooking\",\"nutrition\"]}', 'memory_strategy_id': 'memory_preference_ghc4p-zjtCX1D9Eo', 'namespaces': ['/preferences/user-id-1']}]\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "\n",
      "\n",
      "For tonight's dinner, you could make:\n",
      "\n",
      "A simple yet delicious meal like:\n",
      "- Baked or pan-seared salmon with lemon and herbs\n",
      "- Fluffy rice or quinoa as a side\n",
      "- Roasted seasonal vegetables (like broccoli, asparagus, or bell peppers)\n",
      "\n",
      "If you're in the mood for something different but still nutritious:\n",
      "- Sheet pan chicken with roasted vegetables\n",
      "- Vegetable stir-fry with tofu or shrimp\n",
      "- A hearty bean and vegetable soup with whole grain bread\n",
      "\n",
      "What ingredients do you have on hand today?\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"configurable\": {\n",
    "        \"thread_id\": \"session-id-2\", # New session ID\n",
    "        \"actor_id\": \"user-id-1\", # Same actor ID\n",
    "    }\n",
    "}\n",
    "\n",
    "run_agent(\"Today's a new day, what should I make for dinner tonight?\", config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapping up\n",
    "\n",
    "As you can see, the agent received both pre-model hook context from the user preferences namespace search and was able to search on its own for long term memories in the fact namespace to create a comprehensive answer for the user.\n",
    "\n",
    "The AgentCoreMemoryStore is very flexible and can be implemented in a variety of ways, including pre/post model hooks or just tools themselves with store operations. Used alongside the AgentCoreMemorySaver for checkpointing, both full conversational state and long term insights can be combined to form a complex and intelligent agent system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
