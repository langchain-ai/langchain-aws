{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bedrock AgentCore Memory Store Walkthrough - Long Term Memory w/ Retrieval Tools\n",
    "\n",
    "This sample notebook walks through setup and usage of the Bedrock AgentCore Memory Store with LangGraph. This approach enables saving of conversations to the AgentCore memory API to be later extracted and retrieved, enabling long term memory.\n",
    "\n",
    "### Setup\n",
    "For this notebook you will need:\n",
    "1. An Amazon Web Services development account\n",
    "2. Bedrock Model Access (i.e. Claude 3.7 Sonnet)\n",
    "3. An AgentCore Memory Resource configured (see below section for details)\n",
    "4. Two strategies enabled for the Agent Core Memory resource, `/facts/{actor_id}` semantic search and `/preferences/{actor_id}` user preference search\n",
    "\n",
    "### AgentCore Memory Resource\n",
    "\n",
    "Either in the AWS developer portal or using the boto3 library you must create an [AgentCore Memory Resource](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock-agentcore-control/client/create_memory.html). For this notebook, only two strategies need to be enabled, user preferences and semantic memory. These strategies will automatically run once we save our conversational messages to AgentCore Memory and extract chunks of information that our agent can retrieve later. For more information on long term memory, see the docs here [AgentCore Long Term Memory](https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/long-term-memory.html).\n",
    "\n",
    "Once you have the Memory enabled and in a `ACTIVE` state, take note of the `memoryId` and strategy namespaces, we will need them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain langchain-aws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import LangGraph and LangChain components\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langgraph.store.base import BaseStore\n",
    "import uuid\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the AgentCoreMemoryStore that we will use as a store\n",
    "from langgraph_checkpoint_aws import (\n",
    "    AgentCoreMemoryStore,\n",
    "    create_store_event_tool,\n",
    "    create_search_memory_tool\n",
    ")\n",
    "\n",
    "# For this example, we will just use an InMemorySaver to save context.\n",
    "# In production, we highly recommend the AgentCoreMemorySaver as a checkpointer\n",
    "# which works seamlessly alongside the memory store\n",
    "from langgraph.checkpoint.memory import InMemorySaver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AgentCore Memory Configuration\n",
    "- `REGION` corresponds to the AWS region that your resources are present in, these are passed to the `AgentCoreMemorySaver`.\n",
    "- `MEMORY_ID` corresponds to your top level AgentCore Memory resource. Within this resource we will store messages from multiple users and sessions\n",
    "- `MODEL_ID` this is the bedrock model that will power our LangGraph agent.\n",
    "\n",
    "We will use the `MEMORY_ID` and any additional boto3 client keyword args (in our case, `REGION`) to create the store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = \"us-west-2\"\n",
    "MEMORY_ID = \"memory_ghc4p-Hx6JcCEtH3\"\n",
    "MODEL_ID = \"us.anthropic.claude-3-7-sonnet-20250219-v1:0\"\n",
    "\n",
    "# Initialize the store to enable long term memory saving and retrieval\n",
    "store = AgentCoreMemoryStore(memory_id=MEMORY_ID, region_name=REGION)\n",
    "\n",
    "# Initialize Bedrock LLM\n",
    "llm = init_chat_model(MODEL_ID, model_provider=\"bedrock_converse\", region_name=REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing the messages that we want for long term memory extraction\n",
    "\n",
    "For this notebook, we will only be looking at tools to save and retrieve memories per the agents discretion using tools. More deterministic approaches involve using the pre/post model hooks.\n",
    "\n",
    "For techniques on storing messages in pre/post model hooks rather than with agent tools, see this notebook: [https://github.com/langchain-ai/langchain-aws/blob/main/samples/memory/agentcore_memory_store_long_term_search.ipynb](https://github.com/langchain-ai/langchain-aws/blob/main/samples/memory/agentcore_memory_store_long_term_search.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define our long term memory saving and retrieval tools\n",
    "We will use a tool factory built for the AgentCore memory store so we can define tools for any namespace that we want to search over based on the strategies created for our memory resource. These tool factories take care of the logic under the hood for loading actor_id and thread_id from the runtime configuration, ensuring that a tool is only searching over the namespaces for that current user and session.\n",
    "\n",
    "To accomplish this, similar to how AgentCore memory implements namespace placeholders for {actor_id} and {session_id}, we will also provide these to the tool factory so that it knows how to inject these arguments at runtime.\n",
    "\n",
    "The tools will allow the agent to save messages that are important and search the namespace we specify, in this case the /facts/{actor_id} namespace which is a semantic memory namespace we specified above (at the top of the notebook). As the memories are extracted over time, these will be available to the agent through this tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the memory save tool using the factory\n",
    "# The actor ID and session ID are handled under the hood at runtime to save to the actor and session based on the invocation below\n",
    "save_events_to_memory_tool = create_store_event_tool(\n",
    "    name=\"save_events_to_memory\",\n",
    ")\n",
    "\n",
    "# Create the memory search tool using the factory\n",
    "retrieve_past_conversation_facts_tool = create_search_memory_tool(\n",
    "    namespace=(\"facts\", \"{actor_id}\"), # Placeholder for actor ID, specifying the namespace we defined /facts/{actorId} in AgentCore Memory\n",
    "    instructions=\"Retrieve facts and user preferences about the user that might be helpful in answering vague questions\",\n",
    "    name=\"get_past_conversation_facts\",\n",
    ")\n",
    "\n",
    "tools = [save_events_to_memory_tool, retrieve_past_conversation_facts_tool]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build our LangGraph agent graph\n",
    "\n",
    "Our agent will be built with the `create_react_agent` builder. It just has a few simple nodes, mainly a chatbot node and a tool node. The tool node will contain just our long term memory retrieval tool and the pre and post model hooks are specified as arguments.\n",
    "\n",
    "**Note**: for custom agent implementations the Store and tools can be configured to run as needed for any workflow following this pattern. Pre/post model hooks can be used, the whole conversation could be saved at the end, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = create_react_agent(\n",
    "    llm,\n",
    "    store=store,\n",
    "    tools=tools,\n",
    "    checkpointer=InMemorySaver()\n",
    ")\n",
    "\n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTANT: Input and Config\n",
    "\n",
    "### Graph Invoke Input\n",
    "We only need to pass the newest user message in as an argument `inputs`. This could include other state variables too but for the simple `create_react_agent`, messages are all that's required.\n",
    "\n",
    "### LangGraph RuntimeConfig\n",
    "In LangGraph, config is a `RuntimeConfig` that contains attributes that are necessary at invocation time, for example user IDs or session IDs. For the `AgentCoreMemorySaver`, `thread_id` and `actor_id` must be set in the config. For instance, your AgentCore invocation endpoint could assign this based on the identity or user ID of the caller. Additional documentation here: [https://langchain-ai.github.io/langgraphjs/how-tos/configuration/](https://langchain-ai.github.io/langgraphjs/how-tos/configuration/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"configurable\": {\n",
    "        \"thread_id\": \"session-1\", # REQUIRED: This maps to Bedrock AgentCore session_id under the hood\n",
    "        \"actor_id\": \"usr-1\", # REQUIRED: This maps to Bedrock AgentCore actor_id under the hood\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Agent\n",
    "\n",
    "For this example, we will run through a conversation where the user is talking about what they like to cook with. This will give the backend enough context to extract facts and user preferences that we can retrieve the next time the user asks for what to make on a given evening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to pretty print agent output while running\n",
    "def run_agent(query: str, config: RunnableConfig):\n",
    "    printed_ids = set()\n",
    "    events = graph.stream(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n",
    "        config,\n",
    "        stream_mode=\"values\",\n",
    "    )\n",
    "    for event in events:\n",
    "        if \"messages\" in event:\n",
    "            for msg in event[\"messages\"]:\n",
    "                # Check if we've already printed this message\n",
    "                if id(msg) not in printed_ids:\n",
    "                    msg.pretty_print()\n",
    "                    printed_ids.add(id(msg))\n",
    "\n",
    "\n",
    "prompt = \"\"\"\n",
    "Hey there! Im cooking one of my favorite meals tonight, salmon with rice and veggies (healthy). Has\n",
    "great macros for my weightlifting competition that is coming up. What can I add to this dish to make it taste better\n",
    "and also improve the protein and vitamins I get?\n",
    "\n",
    "Make sure to make note of this for future competitions.\n",
    "\"\"\"\n",
    "\n",
    "run_agent(prompt, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent access to the store\n",
    "\n",
    "**Note** - since AgentCore memory processes these events in the background, it may take a few seconds for the memory to be extracted and embedded to long term memory retrieval.\n",
    "\n",
    "Great! Now we have seen that long term memories were extracted to our namespaces based on the earlier messages in the conversation.\n",
    "\n",
    "Now, let's start a new session and ask about recommendations for what to cook for dinner. The agent can use the store to access the long term memories that were extracted to make a recommendation that the user will be sure to like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"configurable\": {\n",
    "        \"thread_id\": \"session-2\", # New session ID\n",
    "        \"actor_id\": \"usr-1\", # Same actor ID\n",
    "    }\n",
    "}\n",
    "\n",
    "run_agent(\"Today's a new day, what should I make for dinner tonight?\", config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapping up\n",
    "\n",
    "The tool approach is not as deterministic as the pre/post model hook approach, but does give the agent more say in what memories to store and query for during a coversation.\n",
    "\n",
    "Used alongside the AgentCoreMemorySaver for checkpointing, both full conversational state and long term insights can be combined to form a complex and intelligent agent system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
