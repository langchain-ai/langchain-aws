{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8208f7d6-391d-4054-9e67-ac0f85878dcd",
   "metadata": {},
   "source": [
    "# Bedrock AgentCore Memory Checkpointer Walkthrough\n",
    "\n",
    "This sample notebook walks through setup and usage of the Bedrock AgentCore Memory Checkpointer with LangGraph. This approach enables saving of conversations and state data to the Memory API for persistent storage, fault tolerance, and human-in-the-loop workflows.\n",
    "\n",
    "### Setup\n",
    "For this notebook you will need:\n",
    "1. An Amazon Web Services development account\n",
    "2. Bedrock Model Access (i.e. Claude 3.7 Sonnet)\n",
    "3. An AgentCore Memory Resource configured (see below section for details)\n",
    "\n",
    "### AgentCore Memory Resource\n",
    "\n",
    "Either in the AWS developer portal or using the boto3 library you must create an [AgentCore Memory Resource](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock-agentcore-control/client/create_memory.html). For just using the `AgentCoreMemorySaver` checkpointer in this notebook, you do not need to specify any specific long-term memory strategies. However, it may be beneficial to supplement this approach with the `AgentCoreMemoryStore` to save and extract conversational insights, so you may want to enable strategies for that use case.\n",
    "\n",
    "Once you have the Memory enabled and in a `ACTIVE` state, take note of the `memoryId`, we will need it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6acbf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8eff8706-6715-4981-983b-934561ee0a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import LangGraph and LangChain components\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.tools import tool\n",
    "from langgraph.prebuilt import create_react_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc12fd92-b84a-4d2f-b96d-83d3da08bf70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the AgentCoreMemorySaver that we will use as a checkpointer\n",
    "from langgraph_checkpoint_aws import AgentCoreMemorySaver\n",
    "\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d91a14-541c-426c-a52c-5fdcf29f8953",
   "metadata": {},
   "source": [
    "## AgentCore Memory Configuration\n",
    "- `REGION` corresponds to the AWS region that your resources are present in, these are passed to the `AgentCoreMemorySaver`.\n",
    "- `MEMORY_ID` corresponds to your top level AgentCore Memory resource. Within this resource we will store checkpoints for multiple actors and sessions\n",
    "- `MODEL_ID` this is the bedrock model that will power our LangGraph agent through Bedrock Converse.\n",
    "\n",
    "We will use the `MEMORY_ID` and any additional boto3 client keyword args (in our case, `REGION`) to instantiate our checkpointer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "226d094c-a05d-4f88-851d-cc42ff63ef11",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = \"us-west-2\"\n",
    "MEMORY_ID = \"YOUR_MEMORY_ID\"\n",
    "MODEL_ID = \"us.anthropic.claude-3-7-sonnet-20250219-v1:0\"\n",
    "\n",
    "# Initialize checkpointer for state persistence\n",
    "checkpointer = AgentCoreMemorySaver(MEMORY_ID, region_name=REGION)\n",
    "\n",
    "# Initialize LLM\n",
    "llm = init_chat_model(MODEL_ID, model_provider=\"bedrock_converse\", region_name=REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56c77aa-4b19-49a5-af3c-db2b4798384b",
   "metadata": {},
   "source": [
    "## Define our tools\n",
    "\n",
    "For this demo notebook we will only give our agent access to two simple tools, adding and multiplying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd81ba1c-3fb8-474c-ae40-0ee7c62ca234",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def add(a: int, b: int):\n",
    "    \"\"\"Add two integers and return the result\"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "@tool\n",
    "def multiply(a: int, b: int):\n",
    "    \"\"\"Multiply two integers and return the result\"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "tools = [add, multiply]\n",
    "\n",
    "# Bind the tools to our LLM so it can understand their structure\n",
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651b6e3f-8b41-4c07-8a32-680666b2661e",
   "metadata": {},
   "source": [
    "## Build our LangGraph agent graph\n",
    "\n",
    "Our agent will have a few simple nodes, mainly a chatbot node and a tool node. This will enable our chatbot to use the add and multiply tools as much as it needs and then return a response. We will visualize this graph below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a116a57f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x12139bc50>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph = create_react_agent(\n",
    "    model=llm_with_tools,\n",
    "    tools=[add, multiply],\n",
    "    prompt=\"You are a helpful assistant\",\n",
    "    checkpointer=checkpointer,\n",
    ")\n",
    "\n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cbbb3a-f85c-42f5-bbfb-40f5f6b8b48c",
   "metadata": {},
   "source": [
    "## IMPORTANT: Input and Config\n",
    "\n",
    "### Graph Invoke Input\n",
    "We only need to pass the newest user message in as an argument `inputs`. This could include other state variables too but for the simple `create_react_agent`, messages are all that's required.\n",
    "\n",
    "### LangGraph RuntimeConfig\n",
    "In LangGraph, config is a `RuntimeConfig` that contains attributes that are necessary at invocation time, for example user IDs or session IDs. For the `AgentCoreMemorySaver`, `thread_id` and `actor_id` must be set in the config. For instance, your AgentCore invocation endpoint could assign this based on the identity or user ID of the caller. Additional documentation here: [https://langchain-ai.github.io/langgraphjs/how-tos/configuration/](https://langchain-ai.github.io/langgraphjs/how-tos/configuration/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "859fee36-1eff-4713-9c3b-387b702c0301",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"configurable\": {\n",
    "        \"thread_id\": \"session-1\", # REQUIRED: This maps to Bedrock AgentCore session_id under the hood\n",
    "        \"actor_id\": \"react-agent-1\", # REQUIRED: This maps to Bedrock AgentCore actor_id under the hood\n",
    "    }\n",
    "}\n",
    "\n",
    "inputs = {\"messages\": [{\"role\": \"user\", \"content\": \"What is 1337 times 515321? Then add 412 and return the value to me.\"}]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96044de0-2d32-4811-ac30-43f4416f4303",
   "metadata": {},
   "source": [
    "### Run the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2bc4869-58cd-4914-9a7a-8d39ecd16226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent': {'messages': [AIMessage(content=[{'type': 'text', 'text': \"I'll solve this step-by-step using the available tools.\\n\\nFirst, I'll multiply 1337 by 515321:\"}, {'type': 'tool_use', 'name': 'multiply', 'input': {'a': 1337, 'b': 515321}, 'id': 'tooluse_ufONDYV9T_GDk9uYGKrUBA'}], additional_kwargs={}, response_metadata={'ResponseMetadata': {'RequestId': 'c3d3db9c-985f-4c2a-88f0-4aaa5c673900', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Wed, 17 Sep 2025 20:37:20 GMT', 'content-type': 'application/json', 'content-length': '497', 'connection': 'keep-alive', 'x-amzn-requestid': 'c3d3db9c-985f-4c2a-88f0-4aaa5c673900'}, 'RetryAttempts': 0}, 'stopReason': 'tool_use', 'metrics': {'latencyMs': [2280]}, 'model_name': 'us.anthropic.claude-3-7-sonnet-20250219-v1:0'}, id='run--364254f3-1cd7-4394-8d27-b52355454b40-0', tool_calls=[{'name': 'multiply', 'args': {'a': 1337, 'b': 515321}, 'id': 'tooluse_ufONDYV9T_GDk9uYGKrUBA', 'type': 'tool_call'}], usage_metadata={'input_tokens': 482, 'output_tokens': 100, 'total_tokens': 582, 'input_token_details': {'cache_creation': 0, 'cache_read': 0}})]}}\n",
      "{'tools': {'messages': [ToolMessage(content='688984177', name='multiply', id='db6b78b8-165d-44cd-96b5-cd3b21bdc4a6', tool_call_id='tooluse_ufONDYV9T_GDk9uYGKrUBA')]}}\n",
      "{'agent': {'messages': [AIMessage(content=[{'type': 'text', 'text': \"Now I'll add 412 to the result:\"}, {'type': 'tool_use', 'name': 'add', 'input': {'a': 688984177, 'b': 412}, 'id': 'tooluse_PuRt_QohS-2ZW_-Xjp1brQ'}], additional_kwargs={}, response_metadata={'ResponseMetadata': {'RequestId': '8177a4ec-802a-44ca-9323-936e91f3aa69', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Wed, 17 Sep 2025 20:37:22 GMT', 'content-type': 'application/json', 'content-length': '429', 'connection': 'keep-alive', 'x-amzn-requestid': '8177a4ec-802a-44ca-9323-936e91f3aa69'}, 'RetryAttempts': 0}, 'stopReason': 'tool_use', 'metrics': {'latencyMs': [1642]}, 'model_name': 'us.anthropic.claude-3-7-sonnet-20250219-v1:0'}, id='run--80836015-5332-4d24-aeb6-3ad1cebf826f-0', tool_calls=[{'name': 'add', 'args': {'a': 688984177, 'b': 412}, 'id': 'tooluse_PuRt_QohS-2ZW_-Xjp1brQ', 'type': 'tool_call'}], usage_metadata={'input_tokens': 596, 'output_tokens': 83, 'total_tokens': 679, 'input_token_details': {'cache_creation': 0, 'cache_read': 0}})]}}\n",
      "{'tools': {'messages': [ToolMessage(content='688984589', name='add', id='422ab45d-0a3e-4643-9c0b-0cc0073775fc', tool_call_id='tooluse_PuRt_QohS-2ZW_-Xjp1brQ')]}}\n",
      "{'agent': {'messages': [AIMessage(content='The final answer is 688,984,589.\\n\\nFirst I multiplied 1337 by 515321, which equals 688,984,177. Then I added 412 to that result, giving us the final value of 688,984,589.', additional_kwargs={}, response_metadata={'ResponseMetadata': {'RequestId': '7a9cbf35-301a-43e4-97dd-fbb8dd64bcfb', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Wed, 17 Sep 2025 20:37:23 GMT', 'content-type': 'application/json', 'content-length': '465', 'connection': 'keep-alive', 'x-amzn-requestid': '7a9cbf35-301a-43e4-97dd-fbb8dd64bcfb'}, 'RetryAttempts': 0}, 'stopReason': 'end_turn', 'metrics': {'latencyMs': [1785]}, 'model_name': 'us.anthropic.claude-3-7-sonnet-20250219-v1:0'}, id='run--c1484fdb-6b49-4d6b-8248-6fb901808436-0', usage_metadata={'input_tokens': 693, 'output_tokens': 61, 'total_tokens': 754, 'input_token_details': {'cache_creation': 0, 'cache_read': 0}})]}}\n"
     ]
    }
   ],
   "source": [
    "for chunk in graph.stream(inputs, stream_mode=\"updates\", config=config):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d28db8-c9cc-4fa1-922d-89476e9f71c5",
   "metadata": {},
   "source": [
    "## Inspect the current state with AgentCoreMemory\n",
    "\n",
    "Under the hood when you call `graph.get_state(config)` it calls the checkpointer to retrieve the latest checkpoint saved for our actor and session. We can take a look at the values saved at this point in the conversation from `messages`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60042ec5-cd64-48f3-bd7d-cb7ca9e21b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human: What is 1337 times 515321? Then add 412 and return the value to me.\n",
      "=========================================\n",
      "ai: I'll solve this step-by-step using the available tools.\n",
      "\n",
      "First, I'll multiply 1337 by 515321:\n",
      "=========================================\n",
      "tool: 688984177\n",
      "=========================================\n",
      "ai: Now I'll add 412 to the result:\n",
      "=========================================\n",
      "tool: 688984589\n",
      "=========================================\n",
      "ai: The final answer is 688,984,589.\n",
      "\n",
      "First I multiplied 1337 by 515321, which equals 688,984,177. Then I added 412 to that result, giving us the final value of 688,984,589.\n",
      "=========================================\n"
     ]
    }
   ],
   "source": [
    "for message in graph.get_state(config).values.get(\"messages\"):\n",
    "    print(f\"{message.type}: {message.text()}\")\n",
    "    print(\"=========================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa0c9bd-6fb6-467d-b51a-84696238b4fa",
   "metadata": {},
   "source": [
    "## Look at a previous checkpoints during execution\n",
    "Using the `graph.get_state_history(config)` we will see the checkpoints that were saved, then inspect a previous checkpoint's values for `messages`. Checkpoints are listed so the most recent checkpoints appear first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2232c2b-154d-4b50-93b0-925eb88e67c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Checkpoint ID: 1f094061-dc46-6a54-8005-daf53773fe70) # of messages in state: 6\n",
      "(Checkpoint ID: 1f094061-ca8e-63b6-8004-f69f7debce02) # of messages in state: 5\n",
      "(Checkpoint ID: 1f094061-ca73-6598-8003-770972f8d2e2) # of messages in state: 4\n",
      "(Checkpoint ID: 1f094061-ba19-6efe-8002-94a1cb0a0063) # of messages in state: 3\n",
      "(Checkpoint ID: 1f094061-ba06-6f20-8001-ffd58cf51b40) # of messages in state: 2\n",
      "(Checkpoint ID: 1f094061-a168-61b2-8000-8a5fb60f2073) # of messages in state: 1\n",
      "(Checkpoint ID: 1f094061-a145-62de-bfff-db79a2d38f86) # of messages in state: 0\n"
     ]
    }
   ],
   "source": [
    "for checkpoint in graph.get_state_history(config):\n",
    "    print(\n",
    "        f\"(Checkpoint ID: {checkpoint.config['configurable']['checkpoint_id']}) # of messages in state: {len(checkpoint.values.get('messages'))}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab07194f-49b4-4fb9-a74b-d86b970cace6",
   "metadata": {},
   "source": [
    "## Continue the conversation\n",
    "By using the same config with our session and actor from before, we can continue the conversation by invoking our LangGraph agent with a new invocation state. The checkpointer in the background will take care of loading in context so that all the previous messages from the first interaction are loaded in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7117c6a-d1d5-4866-aee9-2b7229fd73fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent': {'messages': [AIMessage(content='The first calculations you asked me to do were to multiply 1337 by 515321, and then add 412 to the result.\\n\\nSpecifically:\\n1. Multiply: 1337 Ã— 515321 = 688,984,177\\n2. Add: 688,984,177 + 412 = 688,984,589\\n\\nI performed these calculations using the multiply and add tools as requested in your initial query.', additional_kwargs={}, response_metadata={'ResponseMetadata': {'RequestId': 'd6d1a259-9195-4d8b-8f3f-92b2043a1f62', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Wed, 17 Sep 2025 20:38:06 GMT', 'content-type': 'application/json', 'content-length': '605', 'connection': 'keep-alive', 'x-amzn-requestid': 'd6d1a259-9195-4d8b-8f3f-92b2043a1f62'}, 'RetryAttempts': 0}, 'stopReason': 'end_turn', 'metrics': {'latencyMs': [2263]}, 'model_name': 'us.anthropic.claude-3-7-sonnet-20250219-v1:0'}, id='run--1f8ee6ce-ed7f-46fc-b7f2-a97cf5440884-0', usage_metadata={'input_tokens': 768, 'output_tokens': 100, 'total_tokens': 868, 'input_token_details': {'cache_creation': 0, 'cache_read': 0}})]}}\n"
     ]
    }
   ],
   "source": [
    "inputs = {\"messages\": [{\"role\": \"user\", \"content\": \"What were the first calculations I asked you to do?\"}]}\n",
    "\n",
    "for chunk in graph.stream(inputs, stream_mode=\"updates\", config=config):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc602c5b-e6b3-4099-820a-a82266039849",
   "metadata": {},
   "source": [
    "## Start a new chat by using a new session ID\n",
    "\n",
    "By providing a new thread_id, a new conversation will be started (while still persisting the old conversation in AgentCore Memory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ce88359-ee1e-44bd-9095-ad22b880dda8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent': {'messages': [AIMessage(content=\"I don't see that you've asked me to multiply or add any specific values yet. While I have tools available to perform addition and multiplication, you haven't provided the numbers to calculate.\\n\\nWould you like me to:\\n1. Multiply two numbers together, or\\n2. Add two numbers together?\\n\\nIf so, please provide the specific values you'd like me to use for the calculation.\", additional_kwargs={}, response_metadata={'ResponseMetadata': {'RequestId': '5a7e1c01-9023-4d23-9491-2d2c46f347a9', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Wed, 17 Sep 2025 20:39:41 GMT', 'content-type': 'application/json', 'content-length': '666', 'connection': 'keep-alive', 'x-amzn-requestid': '5a7e1c01-9023-4d23-9491-2d2c46f347a9'}, 'RetryAttempts': 0}, 'stopReason': 'end_turn', 'metrics': {'latencyMs': [2441]}, 'model_name': 'us.anthropic.claude-3-7-sonnet-20250219-v1:0'}, id='run--58b14eb8-5244-40e9-aa13-6d9ef82f1997-0', usage_metadata={'input_tokens': 470, 'output_tokens': 85, 'total_tokens': 555, 'input_token_details': {'cache_creation': 0, 'cache_read': 0}})]}}\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"configurable\": {\n",
    "        \"thread_id\": \"session-2\", # New session ID\n",
    "        \"actor_id\": \"react-agent-1\", # Same Actor ID\n",
    "    }\n",
    "}\n",
    "\n",
    "inputs = {\"messages\": [{\"role\": \"user\", \"content\": \"What values did I ask you to multiply and add?\"}]}\n",
    "for chunk in graph.stream(inputs, stream_mode=\"updates\", config=config):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa7bd51-0ece-47d6-aae3-2a9041d98645",
   "metadata": {},
   "source": [
    "## Wrapping Up\n",
    "\n",
    "As you can see, the AgentCore checkpointer is very powerful for persisting conversational and graph state in the background. For AgentCore runtime deployments, you must figure out how to determine the session ID and actor ID in the `/invocations` endpoint, for example like so:\n",
    "\n",
    "```python\n",
    "# ... LangGraph create_react_agent logic and checkpoint initialization ...\n",
    "\n",
    "# AgentCore Entrypoint\n",
    "@app.entrypoint\n",
    "def agent_invocation(payload, context):\n",
    "    \"\"\"\n",
    "    AgentCore invocation endpoint that handles incoming requests.\n",
    "    \n",
    "    Sample payload format:\n",
    "    {\n",
    "        \"prompt\": \"user question here\",\n",
    "        \"thread_id\": \"optional-thread-id\",\n",
    "        \"actor_id\": \"optional-actor-id\"\n",
    "    }\n",
    "    \"\"\"\n",
    "    print(\"Received payload:\", payload)\n",
    "    \n",
    "    # Extract prompt from payload\n",
    "    prompt = payload[\"prompt\"]\n",
    "    inputs = {\"messages\": [{\"role\": \"user\", \"content\": prompt}]}\n",
    "    \n",
    "    # Extract or generate thread_id and actor_id\n",
    "    # In production, you might derive these from the context or caller identity\n",
    "    thread_id = payload.get(\"thread_id\")\n",
    "    actor_id = payload.get(\"actor_id\")\n",
    "    \n",
    "    # Create runtime config for invocation\n",
    "    config = {\n",
    "        \"configurable\": {\n",
    "            \"thread_id\": thread_id,\n",
    "            \"actor_id\": actor_id,\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Invoke the graph with checkpointing\n",
    "    output = graph.invoke(inputs, config=config)\n",
    "    \n",
    "    # Return the response\n",
    "    return {\n",
    "        \"result\": output['messages'][-1],\n",
    "        \"thread_id\": thread_id,\n",
    "        \"actor_id\": actor_id,\n",
    "        \"message_count\": len(output['messages'])\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda8649a-d48f-467c-94b9-99dd8fb2c078",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
