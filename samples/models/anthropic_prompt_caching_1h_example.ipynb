{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anthropic 1-Hour Prompt Caching with ChatBedrockConverse\n",
    "\n",
    "This notebook demonstrates how to use extended cache TTL (1 hour) with Anthropic models through AWS Bedrock's Converse API using the `ChatBedrockConverse` integration.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "1. AWS credentials configured\n",
    "2. Access to Anthropic Claude models in AWS Bedrock\n",
    "3. langchain-aws package installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if not already installed\n",
    "# !pip install langchain-aws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws import ChatBedrockConverse\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Initialize ChatBedrockConverse with Beta Header\n",
    "\n",
    "To enable 1-hour prompt caching with Anthropic models, you must include the beta header in `additional_model_request_fields`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model with beta header for extended cache TTL\n",
    "llm = ChatBedrockConverse(\n",
    "    model=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "    region_name=\"us-west-2\",  # Update to your region\n",
    "    additional_model_request_fields={\n",
    "        \"anthropicBeta\": [\"extended-cache-ttl-2025-04-11\"]\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"✓ ChatBedrockConverse initialized with 1-hour caching support\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create Cache Points\n",
    "\n",
    "Cache points define where and how long content should be cached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cache points with different TTLs\n",
    "cache_1h = ChatBedrockConverse.create_cache_point(ttl=\"1h\")     # 1-hour cache\n",
    "cache_5m = ChatBedrockConverse.create_cache_point()              # Default 5-minute cache\n",
    "\n",
    "print(\"1-hour cache point:\", cache_1h)\n",
    "print(\"5-minute cache point:\", cache_5m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Basic Usage with Direct Messages\n",
    "\n",
    "**Important**: 1-hour cache entries must appear before 5-minute cache entries in your message content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create messages with cache points\n",
    "messages = [\n",
    "    HumanMessage(content=[\n",
    "        # Long system prompt that doesn't change often - cache for 1 hour\n",
    "        \"\"\"You are an expert AI assistant with extensive knowledge in multiple domains.\n",
    "        You always provide helpful, accurate, and detailed responses.\n",
    "        You follow these guidelines:\n",
    "        1. Be concise but thorough\n",
    "        2. Use examples when helpful\n",
    "        3. Admit when you don't know something\n",
    "        4. Provide sources when possible\"\"\",\n",
    "        cache_1h,  # Cache the above content for 1 hour\n",
    "        \n",
    "        # Context that might change more frequently - cache for 5 minutes\n",
    "        \"Today's date is January 17, 2025. The user is located in Seattle.\",\n",
    "        cache_5m,  # Cache this for 5 minutes\n",
    "        \n",
    "        # The actual user question (not cached)\n",
    "        \"What's the weather typically like in Seattle in January?\"\n",
    "    ])\n",
    "]\n",
    "\n",
    "response = llm.invoke(messages)\n",
    "print(\"Response:\", response.content[:300] + \"...\")\n",
    "\n",
    "# Check cache usage\n",
    "if hasattr(response, 'usage_metadata') and response.usage_metadata:\n",
    "    print(f\"\\nToken usage:\")\n",
    "    print(f\"  Input tokens: {response.usage_metadata.get('input_tokens', 0)}\")\n",
    "    print(f\"  Output tokens: {response.usage_metadata.get('output_tokens', 0)}\")\n",
    "    \n",
    "    if 'input_token_details' in response.usage_metadata:\n",
    "        details = response.usage_metadata['input_token_details']\n",
    "        print(f\"  Cache read tokens: {details.get('cache_read', 0)}\")\n",
    "        print(f\"  Cache creation tokens: {details.get('cache_creation', 0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Using ChatPromptTemplate with Caching\n",
    "\n",
    "This shows how to integrate prompt caching with LangChain's `ChatPromptTemplate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a template with cached system message\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=[\n",
    "        \"\"\"You are a helpful coding assistant specialized in Python.\n",
    "        You provide clear explanations with practical examples.\n",
    "        Always follow PEP 8 style guidelines in your code examples.\"\"\",\n",
    "        cache_1h  # Cache system prompt for 1 hour\n",
    "    ]),\n",
    "    HumanMessage(content=[\n",
    "        \"Current context: User is learning {topic}\",\n",
    "        cache_5m,  # Cache context for 5 minutes\n",
    "        \"{question}\"  # User's question is not cached\n",
    "    ])\n",
    "])\n",
    "\n",
    "# Create a chain\n",
    "chain = template | llm | StrOutputParser()\n",
    "\n",
    "# First invocation (creates cache)\n",
    "response1 = chain.invoke({\n",
    "    \"topic\": \"async programming\",\n",
    "    \"question\": \"How do I use async/await in Python?\"\n",
    "})\n",
    "\n",
    "print(\"First response (cache created):\")\n",
    "print(response1[:300] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second invocation (uses cache)\n",
    "response2 = chain.invoke({\n",
    "    \"topic\": \"async programming\",  # Same topic, will use 5m cache\n",
    "    \"question\": \"What's the difference between asyncio.run() and asyncio.create_task()?\"\n",
    "})\n",
    "\n",
    "print(\"Second response (using cache):\")\n",
    "print(response2[:300] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Structured Output with Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "# Define output schema\n",
    "class CodeAnalysis(BaseModel):\n",
    "    \"\"\"Analysis of provided code.\"\"\"\n",
    "    language: str = Field(description=\"Programming language\")\n",
    "    issues: List[str] = Field(description=\"List of potential issues\")\n",
    "    improvements: List[str] = Field(description=\"Suggested improvements\")\n",
    "    complexity: str = Field(description=\"Overall complexity: low, medium, or high\")\n",
    "\n",
    "# Create template with caching\n",
    "code_template = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=[\n",
    "        \"\"\"You are a code review expert. Analyze the provided code and identify:\n",
    "        1. The programming language\n",
    "        2. Potential issues or bugs\n",
    "        3. Possible improvements\n",
    "        4. Overall complexity assessment\n",
    "        \n",
    "        Be thorough but concise in your analysis.\"\"\",\n",
    "        cache_1h\n",
    "    ]),\n",
    "    (\"human\", \"Please analyze this code:\\n\\n{code}\")\n",
    "])\n",
    "\n",
    "# Create structured output chain\n",
    "structured_llm = llm.with_structured_output(CodeAnalysis)\n",
    "code_chain = code_template | structured_llm\n",
    "\n",
    "# Example code to analyze\n",
    "code_sample = \"\"\"\n",
    "def fibonacci(n):\n",
    "    if n <= 1:\n",
    "        return n\n",
    "    return fibonacci(n-1) + fibonacci(n-2)\n",
    "\"\"\"\n",
    "\n",
    "# Get structured analysis\n",
    "analysis = code_chain.invoke({\"code\": code_sample})\n",
    "\n",
    "print(f\"Language: {analysis.language}\")\n",
    "print(f\"Issues: {', '.join(analysis.issues)}\")\n",
    "print(f\"Improvements: {', '.join(analysis.improvements)}\")\n",
    "print(f\"Complexity: {analysis.complexity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4: Complex Conversation with Multiple Cache Durations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex template with multiple cached sections\n",
    "# Remember: 1-hour cache must come before 5-minute cache\n",
    "advanced_template = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=[\n",
    "        # Long-term stable instructions (1 hour cache)\n",
    "        \"\"\"You are an AI assistant for a software development team.\n",
    "        Core responsibilities:\n",
    "        - Code review and optimization\n",
    "        - Architecture design\n",
    "        - Best practices guidance\n",
    "        - Performance analysis\n",
    "        \n",
    "        Always consider security, scalability, and maintainability.\"\"\",\n",
    "        cache_1h,\n",
    "        \n",
    "        # Medium-term context (5 minute cache)\n",
    "        \"Current sprint focus: {sprint_focus}\",\n",
    "        cache_5m\n",
    "    ]),\n",
    "    MessagesPlaceholder(variable_name=\"history\", optional=True),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "advanced_chain = advanced_template | llm | StrOutputParser()\n",
    "\n",
    "# First call\n",
    "response1 = advanced_chain.invoke({\n",
    "    \"sprint_focus\": \"API optimization and database query performance\",\n",
    "    \"history\": [],\n",
    "    \"question\": \"What are the best practices for optimizing PostgreSQL queries?\"\n",
    "})\n",
    "\n",
    "print(\"Advanced example - First response:\")\n",
    "print(response1[:300] + \"...\")\n",
    "\n",
    "# Second call with history\n",
    "response2 = advanced_chain.invoke({\n",
    "    \"sprint_focus\": \"API optimization and database query performance\",\n",
    "    \"history\": [\n",
    "        HumanMessage(\"What are the best practices for optimizing PostgreSQL queries?\"),\n",
    "        AIMessage(response1)\n",
    "    ],\n",
    "    \"question\": \"How do I implement these optimizations in SQLAlchemy?\"\n",
    "})\n",
    "\n",
    "print(\"\\nAdvanced example - Second response (using cache):\")\n",
    "print(response2[:300] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Cache Effectiveness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a consistent template for testing cache effectiveness\n",
    "test_template = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=[\n",
    "        \"\"\"You are an expert data scientist with deep knowledge of machine learning algorithms.\n",
    "        Provide detailed explanations with mathematical foundations when appropriate.\n",
    "        Always include practical implementation tips.\"\"\",\n",
    "        cache_1h\n",
    "    ]),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "test_chain = test_template | llm | StrOutputParser()\n",
    "\n",
    "# Multiple questions using the same cached system prompt\n",
    "questions = [\n",
    "    \"Explain gradient descent in simple terms.\",\n",
    "    \"What is the difference between supervised and unsupervised learning?\",\n",
    "    \"How does a neural network learn?\"\n",
    "]\n",
    "\n",
    "responses = []\n",
    "for i, question in enumerate(questions, 1):\n",
    "    print(f\"\\nQuestion {i}: {question}\")\n",
    "    response = test_chain.invoke({\"question\": question})\n",
    "    responses.append(response)\n",
    "    print(f\"Response {i}: {response[:150]}...\")\n",
    "    \n",
    "    # Check for cache usage (after first call)\n",
    "    if i > 1:\n",
    "        print(\"  → Should be using cached system prompt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### Setup Requirements\n",
    "1. **Beta Header**: Include `{\"anthropicBeta\": [\"extended-cache-ttl-2025-04-11\"]}` in `additional_model_request_fields`\n",
    "2. **Anthropic Models**: Only works with Anthropic Claude models through AWS Bedrock\n",
    "\n",
    "### Cache Point Creation\n",
    "- `ChatBedrockConverse.create_cache_point(ttl=\"1h\")` for 1-hour caching\n",
    "- `ChatBedrockConverse.create_cache_point()` for default 5-minute caching\n",
    "\n",
    "### Message Format\n",
    "Add cache points as list items in message content:\n",
    "```python\n",
    "SystemMessage(content=[\"Your prompt text\", cache_point])\n",
    "```\n",
    "\n",
    "### Important Rules\n",
    "1. **Ordering**: 1-hour cache entries must appear before 5-minute cache entries\n",
    "2. **Best Practices**:\n",
    "   - Cache stable, reusable content (system prompts, instructions)\n",
    "   - Use 1-hour cache for content that rarely changes\n",
    "   - Use 5-minute cache for session-specific context\n",
    "   - Don't cache user-specific or frequently changing content\n",
    "\n",
    "### Integration\n",
    "- Works seamlessly with `ChatPromptTemplate`\n",
    "- Compatible with structured output\n",
    "- Supports conversation history\n",
    "- Can be used in LangChain chains\n",
    "\n",
    "### Monitoring\n",
    "- Check cache usage through `response.usage_metadata`\n",
    "- Monitor `cache_read` and `cache_creation` tokens\n",
    "- Significant cost and latency savings for repeated calls"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}