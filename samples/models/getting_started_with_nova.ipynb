{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d932ae5-c2b1-4de8-865f-aa28509c33ee",
   "metadata": {},
   "source": [
    "## Getting Started with Amazon Nova Models\n",
    "\n",
    "The following examples will show how you can begin integrating with the Amazon Nova models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42115341-e153-45f8-9d3e-d2971f722c4b",
   "metadata": {},
   "source": [
    "## Model Invocation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e03481-0e86-475d-b84a-dba277ba7eb4",
   "metadata": {},
   "source": [
    "#### Text Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f0bb9a-df70-484e-9823-1b474363678b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws import ChatBedrock\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "llm = ChatBedrock(\n",
    "    model_id=\"us.amazon.nova-lite-v1:0\",\n",
    "    model_kwargs=dict(temperature=0.7)\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    (\"system\", \"Provide three alternative song titles for a given user title\"),\n",
    "    (\"user\", \"Teardrops on My Guitar\"),\n",
    "]\n",
    "\n",
    "response = llm.invoke(messages)\n",
    "print(f\"Request ID: {response.id}\")\n",
    "response.pretty_print()\n",
    "\n",
    "\n",
    "# Here we can pass the chat history to the model to ask follow up questions\n",
    "multi_turn_messages = [\n",
    "    *messages,\n",
    "    response,\n",
    "    HumanMessage(content=\"Select your favorite and tell me why\"),\n",
    "]\n",
    "\n",
    "response = llm.invoke(multi_turn_messages)\n",
    "print(f\"\\n\\nRequest ID: {response.id}\")\n",
    "response.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace6d076-16dd-4285-b8cc-862f1c6e0e0d",
   "metadata": {},
   "source": [
    "#### Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f1db1f-01a4-43da-af72-a3a50954a8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws import ChatBedrock\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = ChatBedrock(\n",
    "    model_id=\"us.amazon.nova-lite-v1:0\",\n",
    "    model_kwargs=dict(temperature=0.7)\n",
    ")\n",
    "\n",
    "chain = llm | StrOutputParser()\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are an author with experience writing creative novels\"),\n",
    "    HumanMessage(\n",
    "        content=\"Write an outlin for a novel about a wizard named Theodore graduating from college\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "for chunk in chain.stream(messages):\n",
    "    print(chunk, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fb633d-a49c-4a4b-9869-0fdb995ca8dc",
   "metadata": {},
   "source": [
    "## Agent Workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6dbb61d-ec8e-489a-a012-fad5c6a35341",
   "metadata": {},
   "source": [
    "#### Binding Tools\n",
    "\n",
    "We recommend when taking advantage of tool calling or agentic workflows to use greedy decoding values. For the Nova models, this means temperature=1, topP=1, topK=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288f6941-ebc7-45b0-b6dc-9fe201cc2197",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws import ChatBedrock\n",
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two numbers.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "tools = [multiply]\n",
    "\n",
    "llm_with_tools = ChatBedrock(\n",
    "    model_id=\"us.amazon.nova-lite-v1:0\",\n",
    "    model_kwargs=dict(temperature=1, top_p=1, additional_model_request_fields={\n",
    "        \"inferenceConfig\": {\n",
    "            \"topK\": 1\n",
    "        }\n",
    "    })\n",
    ").bind_tools(tools)\n",
    "\n",
    "response = llm_with_tools.invoke([(\"user\", \"What is 8*8\")])\n",
    "\n",
    "print(\"[Model Response]\\n\")\n",
    "print(response.content)\n",
    "\n",
    "print(\"\\n[Tool Calls]\\n\")\n",
    "print(response.tool_calls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6f0cca-d9d8-4b1e-93f5-e3b8321ea754",
   "metadata": {},
   "source": [
    "#### Tool Calling Agents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e07d53-8381-44cb-a7de-b6e3ffcd1ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import tool, AgentExecutor, create_tool_calling_agent\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_aws import ChatBedrock\n",
    "\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two numbers.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "tools = [multiply]\n",
    "\n",
    "llm_with_tools = ChatBedrock(\n",
    "    model_id=\"us.amazon.nova-lite-v1:0\",\n",
    "    model_kwargs=dict(temperature=1, top_p=1, additional_model_request_fields={\n",
    "        \"inferenceConfig\": {\n",
    "            \"topK\": 1\n",
    "        }\n",
    "    })\n",
    ").bind_tools(tools)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant\",\n",
    "        ),\n",
    "        (\"placeholder\", \"{chat_history}\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"placeholder\", \"{agent_scratchpad}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "agent = create_tool_calling_agent(llm, tools, prompt)\n",
    "\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "agent_executor.invoke({\"input\": \"What is 2*2?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e1ebb6",
   "metadata": {},
   "source": [
    "## Structured Output\n",
    "\n",
    "Structured output is a great way to force the model to return in a specific way. We use Greedy Decoding params here for more determinist results (Temperature = 1, Top P = 1, Top K = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7841a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_aws import ChatBedrock\n",
    "\n",
    "class Joke(BaseModel):\n",
    "    \"\"\"A joke to respond to the user\"\"\"\n",
    "    setup: str = Field(description=\"The setup of the joke\")\n",
    "    punchline: str = Field(description=\"The punchline to the joke\")\n",
    "\n",
    "llm = ChatBedrock(\n",
    "    model=\"us.amazon.nova-lite-v1:0\",\n",
    "    model_kwargs=dict(temperature=1, top_p=1, additional_model_request_fields={\n",
    "        \"inferenceConfig\": {\n",
    "            \"topK\": 1\n",
    "        }\n",
    "    })\n",
    ")\n",
    "\n",
    "structured_llm = llm.with_structured_output(Joke)\n",
    "structured_llm.invoke(\"Tell me a joke about cats\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agi-dev-3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
