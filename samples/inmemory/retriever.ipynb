{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401be7ee-e489-4674-909e-c0a88fcfaa3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import redis\n",
    "import os\n",
    "from langchain_aws.vectorstores.inmemorydb import InMemoryVectorStore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1f7bb9-71ef-4fa9-9f1b-c189d1d901e4",
   "metadata": {},
   "source": [
    "### We will be using the Titan Embeddings Model to generate our Embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67214e00-fe5f-4a56-b075-226f6c9e6735",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws import ChatBedrock\n",
    "from langchain_aws.embeddings import BedrockEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d053a921-c35f-4136-ae97-3f7bafc05f05",
   "metadata": {},
   "source": [
    "###  Define  the Anthropic Model params "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c71ae16-3c70-4e72-bab8-19055a59ec78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Anthropic Model\n",
    "model_kwargs = {\n",
    "    \"temperature\": 0,\n",
    "    \"top_k\": 250,\n",
    "    \"top_p\": 1,\n",
    "    \"stop_sequences\": [\"\\\\n\\\\nHuman:\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f652d49b-12d1-4b86-bb6d-40ea688348e7",
   "metadata": {},
   "source": [
    "### Initialize large language model and use model properties for Claude-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506feecd-defd-47a8-b401-c0e0bc8a5552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the Anthropic Claude model\n",
    "llm = ChatBedrock(\n",
    "    model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\", model_kwargs=model_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d52cc35-0fc6-4014-be8d-605a691ec40c",
   "metadata": {},
   "source": [
    "### Define titan embeddings model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e96e6ba-0d46-4f51-bccb-cdebd3e2b36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Titan Embeddings client\n",
    "embeddings = BedrockEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62059f35-316b-46d4-a5b3-47d2150b8cc8",
   "metadata": {},
   "source": [
    "##### Here is the document we load for using in context. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da16e43-08b7-46b7-a302-f7ba72332587",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"./memorydb-guide.pdf\"\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25b96ee-918e-4082-a46d-fe7a38f26d32",
   "metadata": {},
   "source": [
    "### Pre process the data to split into chunks that can be loaded into Vector database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27de590b-af80-430b-830e-a2a13f3b0704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.6 s, sys: 121 ms, total: 12.8 s\n",
      "Wall time: 12.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "loader = PyPDFLoader(file_path=pdf_path)  # Load the pdf file\n",
    "pages = loader.load_and_split()\n",
    "# pages[10] # Uncomment if you want to see the data\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(  # Create a text splitter\n",
    "    separators=[\n",
    "        \"\\n\\n\",\n",
    "        \"\\n\",\n",
    "        \".\",\n",
    "        \" \",\n",
    "    ],  # Split chunks at (1) paragraph, (2) line, (3) sentence, or (4) word, in that order\n",
    "    chunk_size=1000,  # Divide into 1000-character chunks using the separators above\n",
    "    chunk_overlap=100,  # Number of characters that can overlap with previous chunk\n",
    ")\n",
    "chunks = loader.load_and_split(text_splitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dceb6375-40f1-4e51-87d6-63e6f23274a5",
   "metadata": {},
   "source": [
    "## Using MemoryDB as Vector store. \n",
    "We test out both Semantic Search and using MemoryDB as retriever for RAG. \n",
    "We are using MemoryDB for our vector store. This code tests connection to MemDB and clears the existing data. \n",
    "\n",
    "**Comment client_devo.flushall() if you dont want to clear the data and index creation**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a60937-ba4b-4c07-b122-981c940e4c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from redis.cluster import RedisCluster as MemoryDBCluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9720f0ef-6d71-49fc-ace4-383fa9bba68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = \"<your-cluster-endpoint>\"\n",
    "\n",
    "rc = MemoryDBCluster(\n",
    "    host=f\"{endpoint}\", port=6379, ssl=True, decode_responses=True, ssl_cert_reqs=\"none\"\n",
    ")\n",
    "\n",
    "rc.ping()\n",
    "rc.flushall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20584947-0d4d-4cfc-a8bc-5f8b9d8dac09",
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_NAME = \"idx:vss-mm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b19be4e-fef7-4168-97e1-0902d1b2b323",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_schema = {\"algorithm\": \"HNSW\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b295753-85d7-4bc5-a608-856175b93626",
   "metadata": {},
   "source": [
    "### Create the index and Load the documents with their embeddings into Redis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3378a8c-184b-48ff-9547-c0d9c5a7cb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "vds = InMemoryVectorStore.from_documents(\n",
    "    chunks,\n",
    "    embeddings,\n",
    "    redis_url=f\"rediss://{endpoint}:6379/ssl=True&ssl_cert_reqs=none\",\n",
    "    vector_schema=vector_schema,\n",
    "    index_name=INDEX_NAME,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982d2d65-fe7b-406e-8a8e-d1c94dfc2c0c",
   "metadata": {},
   "source": [
    "### Lets inspect the index we created "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276cf1bd-c529-4efc-8bf4-68ae88ea7701",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "info = rc.ft(INDEX_NAME).info()\n",
    "num_docs = info[\"num_docs\"]\n",
    "space_usage = info[\"space_usage\"]\n",
    "num_indexed_vectors = info[\"num_indexed_vectors\"]\n",
    "vector_space_usage = info[\"vector_space_usage\"]\n",
    "\n",
    "print(\n",
    "    f\"{num_docs} documents ({space_usage} space used vectors indexed {num_indexed_vectors} vector space usage in {vector_space_usage}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febe916e-791f-47ce-8e45-a713e747a45a",
   "metadata": {},
   "source": [
    "### Testing similarity search \n",
    "\n",
    "[Here are some search functions](https://python.langchain.com/docs/integrations/vectorstores/redis#querying)!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e9193e-0a2c-4ea8-9ae9-19180aa19cfb",
   "metadata": {},
   "source": [
    "## Run this if the index is already created and data is loaded "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a785316-4dba-4b51-bf85-8daa816dc236",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "vds = InMemoryVectorStore(\n",
    "    redis_url=f\"rediss://{endpoint}:6379/ssl=True&ssl_cert_reqs=none\",\n",
    "    index_name=INDEX_NAME,\n",
    "    embedding=embeddings,\n",
    "    index_schema=vector_schema,  # Replace with your index schema if needed\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf107321-ed09-4070-8b90-ff0a62a3f0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How to perform backups with memoryDB?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0baf8e-b590-40f6-a6ac-907a13b5ab28",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "results = vds.similarity_search(query)\n",
    "(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc550743-9462-4190-9bc7-d191cc76a847",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in results:\n",
    "    print(item.page_content, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0db49f2-d423-447d-aa0d-4165cdfddc20",
   "metadata": {},
   "source": [
    "## RAG "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48d00bb-a9c4-4dfa-a835-d7ccf88d2489",
   "metadata": {},
   "source": [
    "### RAG\n",
    "The below code helps implement MemoryDB vector database as a retriever. By default, it will use [**semantic similarity**](https://python.langchain.com/docs/integrations/vectorstores/redis#redis-as-retriever)!\n",
    "\n",
    "[Retreival Augmented Generation](https://docs.aws.amazon.com/sagemaker/latest/dg/jumpstart-foundation-models-customize-rag.html)!\n",
    "\n",
    "We will use MemoryDB developer Guide "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07421f16-a538-4b80-b8fa-7a24fafd1db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "retriever = vds.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5c4371-af9a-4adf-b254-ddca9659d11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_prompt = (\n",
    "    \"Use the given context to answer the question. \"\n",
    "    \"If you don't know the answer, say you don't know. \"\n",
    "    \"Use three sentence maximum and keep the answer concise. \"\n",
    "    \"Context: {context}\"\n",
    ")\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "query = \"How do i create a MemoryDB cluster?\"\n",
    "response = chain.invoke({\"input\": query})\n",
    "print(response[\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-aws",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
