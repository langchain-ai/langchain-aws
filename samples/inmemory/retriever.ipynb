{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401be7ee-e489-4674-909e-c0a88fcfaa3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import redis\n",
    "import os\n",
    "from langchain_aws.vectorstores.inmemorydb import InMemoryVectorStore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1f7bb9-71ef-4fa9-9f1b-c189d1d901e4",
   "metadata": {},
   "source": [
    "### We will be using the Titan Embeddings Model to generate our Embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67214e00-fe5f-4a56-b075-226f6c9e6735",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws.embeddings import BedrockEmbeddings\n",
    "from langchain_aws import BedrockLLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d053a921-c35f-4136-ae97-3f7bafc05f05",
   "metadata": {},
   "source": [
    "###  Define  the Anthropic Model params "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c71ae16-3c70-4e72-bab8-19055a59ec78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the Anthropic Model\n",
    "model_kwargs = {\n",
    "    \"max_tokens_to_sample\": 8000,\n",
    "    \"temperature\": 0, \n",
    "    \"top_k\": 250, \n",
    "    \"top_p\": 1,\n",
    "    \"stop_sequences\": [\"\\\\n\\\\nHuman:\"]\n",
    "}    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f652d49b-12d1-4b86-bb6d-40ea688348e7",
   "metadata": {},
   "source": [
    "### Initialize large language model and use model properties for Claude-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506feecd-defd-47a8-b401-c0e0bc8a5552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the Anthropic Claude model\n",
    "llm = BedrockLLM(\n",
    "    model_id=\"anthropic.claude-v2\",\n",
    "    model_kwargs=model_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d52cc35-0fc6-4014-be8d-605a691ec40c",
   "metadata": {},
   "source": [
    "### Define titan embeddings model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e96e6ba-0d46-4f51-bccb-cdebd3e2b36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Titan Embeddings client\n",
    "embeddings = BedrockEmbeddings()\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62059f35-316b-46d4-a5b3-47d2150b8cc8",
   "metadata": {},
   "source": [
    "##### Here is the document we load for using in context. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da16e43-08b7-46b7-a302-f7ba72332587",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"./memorydb-guide.pdf\"\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53292bc5-7d43-42d2-adb1-0353693e00dd",
   "metadata": {},
   "source": [
    "[Go to Section Title](#section_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25b96ee-918e-4082-a46d-fe7a38f26d32",
   "metadata": {},
   "source": [
    "### Pre process the data to split into chunks that can be loaded into Vector database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27de590b-af80-430b-830e-a2a13f3b0704",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "loader = PyPDFLoader(file_path=pdf_path) #load the pdf file\n",
    "pages = loader.load_and_split()\n",
    "# pages[10] # Uncomment if you want to see the data \n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter( #create a text splitter\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \"], #split chunks at (1) paragraph, (2) line, (3) sentence, or (4) word, in that order\n",
    "        chunk_size=1000, #divide into 1000-character chunks using the separators above\n",
    "        chunk_overlap=100 #number of characters that can overlap with previous chunk\n",
    "    )\n",
    "chunks = loader.load_and_split(text_splitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dceb6375-40f1-4e51-87d6-63e6f23274a5",
   "metadata": {},
   "source": [
    "## Using MemoryDB as Vector store. \n",
    "We test out both Semantic Search and using MemoryDB as retriever for RAG. \n",
    "We are using MemoryDB for our vector store. This code tests connection to MemDB and clears the existing data. \n",
    "\n",
    "**Comment client_devo.flushall() if you dont want to clear the data and index creation**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a60937-ba4b-4c07-b122-981c940e4c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from redis.cluster import RedisCluster as MemoryDBCluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9720f0ef-6d71-49fc-ace4-383fa9bba68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = \"<your-cluster-endpoint>\"\n",
    "\n",
    "rc = MemoryDBCluster(host=f\"{endpoint}\", \n",
    "           port=6379,ssl=True, decode_responses=True, ssl_cert_reqs=\"none\")\n",
    "\n",
    "rc.ping()\n",
    "rc.flushall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20584947-0d4d-4cfc-a8bc-5f8b9d8dac09",
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_NAME='idx:vss-mm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b19be4e-fef7-4168-97e1-0902d1b2b323",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_schema = {\n",
    "    \"algorithm\": \"HNSW\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b295753-85d7-4bc5-a608-856175b93626",
   "metadata": {},
   "source": [
    "### Create the index and Load the documents with their embeddings into Redis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3378a8c-184b-48ff-9547-c0d9c5a7cb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "vds = InMemoryVectorStore.from_documents(\n",
    "            chunks,\n",
    "            embeddings,\n",
    "            redis_url=f\"rediss://{endpoint}:6379/ssl=True&ssl_cert_reqs=none\",\n",
    "            vector_schema=vector_schema,\n",
    "            index_name=INDEX_NAME,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982d2d65-fe7b-406e-8a8e-d1c94dfc2c0c",
   "metadata": {},
   "source": [
    "### Lets inspect the index we created "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276cf1bd-c529-4efc-8bf4-68ae88ea7701",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "info = rc.ft(INDEX_NAME).info()\n",
    "num_docs = info['num_docs']\n",
    "space_usage = info['space_usage']\n",
    "num_indexed_vectors = info['num_indexed_vectors']\n",
    "vector_space_usage = (info['vector_space_usage'])\n",
    "\n",
    "print(f\"{num_docs} documents ({space_usage} space used vectors indexed {num_indexed_vectors} vector space usage in {vector_space_usage}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febe916e-791f-47ce-8e45-a713e747a45a",
   "metadata": {},
   "source": [
    "### Testing similarity search \n",
    "\n",
    "[Here are some search functions](https://python.langchain.com/docs/integrations/vectorstores/redis#querying)!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e9193e-0a2c-4ea8-9ae9-19180aa19cfb",
   "metadata": {},
   "source": [
    "## Run this if the index is already created and data is loaded "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a785316-4dba-4b51-bf85-8daa816dc236",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "vds = InMemoryVectorStore(\n",
    "        redis_url=f\"rediss://{endpoint}:6379/ssl=True&ssl_cert_reqs=none\",\n",
    "        index_name=INDEX_NAME,\n",
    "        embedding=embeddings,\n",
    "       # index_schema=index_schema,  # Replace with your index schema if needed\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf107321-ed09-4070-8b90-ff0a62a3f0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"how to do backups with memoryDB?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0baf8e-b590-40f6-a6ac-907a13b5ab28",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "results = vds.similarity_search(query)\n",
    "(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc550743-9462-4190-9bc7-d191cc76a847",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in results:\n",
    "    print(item.page_content, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0db49f2-d423-447d-aa0d-4165cdfddc20",
   "metadata": {},
   "source": [
    "## RAG "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48d00bb-a9c4-4dfa-a835-d7ccf88d2489",
   "metadata": {},
   "source": [
    "### RAG\n",
    "The below code helps implement MemoryDB vector database as a retriever. By default, it will use [**semantic similarity**](https://python.langchain.com/docs/integrations/vectorstores/redis#redis-as-retriever)!\n",
    "\n",
    "[Retreival Augmented Generation](https://docs.aws.amazon.com/sagemaker/latest/dg/jumpstart-foundation-models-customize-rag.html)!\n",
    "\n",
    "We will use MemoryDB developer Guide "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07421f16-a538-4b80-b8fa-7a24fafd1db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "retriever = vds.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d49a35-910d-4ef0-a2fd-321a8953ff09",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"Human: Use the following pieces of context to provide a concise answer in English to the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Assistant:\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "qa_prompt = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": PROMPT},\n",
    "    #verbose=\"true\"\n",
    ")\n",
    "query = \"How do i create a MemoryDB cluster?\"\n",
    "result = qa_prompt({\"query\": query})\n",
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc188cc6-b8c0-44ce-9049-675709187a3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
